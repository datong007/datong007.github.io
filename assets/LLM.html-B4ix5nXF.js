import{_ as i,r as o,o as s,c as r,a as e,b as t,d as a,e as l}from"./app-B_NdI3hB.js";const c={},h=l('<h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm"><span>LLM</span></a></h1><h2 id="_2024-04-11" tabindex="-1"><a class="header-anchor" href="#_2024-04-11"><span>2024-04-11</span></a></h2><h4 id="leveraging-large-language-models-llms-to-support-collaborative-human-ai-online-risk-data-annotation" tabindex="-1"><a class="header-anchor" href="#leveraging-large-language-models-llms-to-support-collaborative-human-ai-online-risk-data-annotation"><span>Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation</span></a></h4><p><strong>Authors</strong>: Jinkyung Park, Pamela Wisniewski, Vivek Singh</p>',4),d=e("strong",null,"Link",-1),u={href:"http://arxiv.org/abs/2404.07926v1",target:"_blank",rel:"noopener noreferrer"},g=e("p",null,[e("strong",null,"Abstract"),t(": In this position paper, we discuss the potential for leveraging LLMs as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating large-scale and complex data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized. Therefore, we provide some of the early benefits and challenges of using LLMs-based tools for risk annotation and suggest future directions for the HCI research community to leverage LLMs as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the LLMs as Research Tools workshop to identify ongoing applications and challenges of using LLMs to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how LLMs can help reshape the HCI community's methods for working with data.")],-1),p=e("h4",{id:"amplegcg-learning-a-universal-and-transferable-generative-model-of-adversarial-suffixes-for-jailbreaking-both-open-and-closed-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#amplegcg-learning-a-universal-and-transferable-generative-model-of-adversarial-suffixes-for-jailbreaking-both-open-and-closed-llms"},[e("span",null,"AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs")])],-1),m=e("p",null,[e("strong",null,"Authors"),t(": Zeyi Liao, Huan Sun")],-1),f=e("strong",null,"Link",-1),v={href:"http://arxiv.org/abs/2404.07921v1",target:"_blank",rel:"noopener noreferrer"},b=e("p",null,[e("strong",null,"Abstract"),t(": As large language models (LLMs) become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned LLMs. In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100% attack success rate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source LLMs, achieving a 99% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source LLMs to closed-source LLMs. In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.")],-1),y=e("h4",{id:"oda-observation-driven-agent-for-integrating-llms-and-knowledge-graphs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#oda-observation-driven-agent-for-integrating-llms-and-knowledge-graphs"},[e("span",null,"ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs")])],-1),L=e("p",null,[e("strong",null,"Authors"),t(": Lei Sun, Zhengwei Tao, Youdi Li, Hiroshi Arakawa")],-1),w=e("strong",null,"Link",-1),_={href:"http://arxiv.org/abs/2404.07677v1",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,[e("strong",null,"Abstract"),t(": The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.")],-1),x=e("h4",{id:"medical-mt5-an-open-source-multilingual-text-to-text-llm-for-the-medical-domain",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#medical-mt5-an-open-source-multilingual-text-to-text-llm-for-the-medical-domain"},[e("span",null,"Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain")])],-1),M=e("p",null,[e("strong",null,"Authors"),t(": Iker García-Ferrero, Rodrigo Agerri, Aitziber Atutxa Salazar, Elena Cabrio, Iker de la Iglesia, Alberto Lavelli, Bernardo Magnini, Benjamin Molinet, Johana Ramirez-Romero, German Rigau, Jose Maria Villa-Gonzalez, Serena Villata, Andrea Zaninello")],-1),A=e("strong",null,"Link",-1),T={href:"http://arxiv.org/abs/2404.07613v1",target:"_blank",rel:"noopener noreferrer"},C=e("p",null,[e("strong",null,"Abstract"),t(": Research on language technology for the development of medical applications is currently a hot topic in Natural Language Understanding and Generation. Thus, a number of large language models (LLMs) have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction. While these LLMs display competitive performance on automated medical texts benchmarks, they have been pre-trained and evaluated with a focus on a single language (English mostly). This is particularly true of text-to-text models, which typically require large amounts of domain-specific pre-training data, often not easily accessible for many languages. In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish. This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we present two new evaluation benchmarks for all four languages with the aim of facilitating multilingual research in this domain. A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian benchmarks, while being competitive with current state-of-the-art LLMs in English.")],-1),S=e("h4",{id:"ultraeval-a-lightweight-platform-for-flexible-and-comprehensive-evaluation-for-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ultraeval-a-lightweight-platform-for-flexible-and-comprehensive-evaluation-for-llms"},[e("span",null,"UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs")])],-1),I=e("p",null,[e("strong",null,"Authors"),t(": Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, Maosong Sun")],-1),z=e("strong",null,"Link",-1),E={href:"http://arxiv.org/abs/2404.07584v1",target:"_blank",rel:"noopener noreferrer"},q=e("p",null,[e("strong",null,"Abstract"),t(": Evaluation is pivotal for honing Large Language Models (LLMs), pinpointing their capabilities and guiding enhancements. The rapid development of LLMs calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher's workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, prompts, and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration. UltraEval is now available for researchers publicly~\\footnote{Website is at \\url{https://github.com/OpenBMB/UltraEval}}.")],-1),P=e("h4",{id:"decomposing-label-space-format-and-discrimination-rethinking-how-llms-respond-and-solve-tasks-via-in-context-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#decomposing-label-space-format-and-discrimination-rethinking-how-llms-respond-and-solve-tasks-via-in-context-learning"},[e("span",null,"Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning")])],-1),W=e("p",null,[e("strong",null,"Authors"),t(": Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan")],-1),G=e("strong",null,"Link",-1),R={href:"http://arxiv.org/abs/2404.07546v1",target:"_blank",rel:"noopener noreferrer"},H=e("p",null,[e("strong",null,"Abstract"),t(": In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaled-up large language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of ICL into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose LLMs across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, ICL exhibits significant efficacy in regulating the label space and format which helps LLMs to respond in desired label words. We then demonstrate this ability functions similar to detailed instructions for LLMs to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with ICL and find that retrieving the most semantically similar examples notably boosts model's discriminative capability.")],-1),B=e("h4",{id:"wese-weak-exploration-to-strong-exploitation-for-llm-agents",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#wese-weak-exploration-to-strong-exploitation-for-llm-agents"},[e("span",null,"WESE: Weak Exploration to Strong Exploitation for LLM Agents")])],-1),D=e("p",null,[e("strong",null,"Authors"),t(": Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen")],-1),O=e("strong",null,"Link",-1),F={href:"http://arxiv.org/abs/2404.07456v1",target:"_blank",rel:"noopener noreferrer"},j=e("p",null,[e("strong",null,"Abstract"),t(": Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent's reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.")],-1),K=e("h4",{id:"learning-to-localize-objects-improves-spatial-reasoning-in-visual-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-to-localize-objects-improves-spatial-reasoning-in-visual-llms"},[e("span",null,"Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs")])],-1),Y=e("p",null,[e("strong",null,"Authors"),t(": Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, Tsung-Yu Lin")],-1),N=e("strong",null,"Link",-1),Z={href:"http://arxiv.org/abs/2404.07449v1",target:"_blank",rel:"noopener noreferrer"},J=e("p",null,[e("strong",null,"Abstract"),t(": Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.")],-1),V=e("h2",{id:"_2024-04-10",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-10"},[e("span",null,"2024-04-10")])],-1),U=e("h4",{id:"biscuit-scaffolding-llm-generated-code-with-ephemeral-uis-in-computational-notebooks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#biscuit-scaffolding-llm-generated-code-with-ephemeral-uis-in-computational-notebooks"},[e("span",null,"BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks")])],-1),Q=e("p",null,[e("strong",null,"Authors"),t(": Ruijia Cheng, Titus Barik, Alan Leung, Fred Hohman, Jeffrey Nichols")],-1),X=e("strong",null,"Link",-1),$={href:"http://arxiv.org/abs/2404.07387v1",target:"_blank",rel:"noopener noreferrer"},ee=e("p",null,[e("strong",null,"Abstract"),t(": Novices frequently engage with machine learning tutorials in computational notebooks and have been adopting code generation technologies based on large language models (LLMs). However, they encounter difficulties in understanding and working with code produced by LLMs. To mitigate these challenges, we introduce a novel workflow into computational notebooks that augments LLM-based code generation with an additional ephemeral UI step, offering users UI-based scaffolds as an intermediate stage between user prompts and code generation. We present this workflow in BISCUIT, an extension for JupyterLab that provides users with ephemeral UIs generated by LLMs based on the context of their code and intentions, scaffolding users to understand, guide, and explore with LLM-generated code. Through 10 user studies where novices used BISCUIT for machine learning tutorials, we discover that BISCUIT offers user semantic representation of code to aid their understanding, reduces the complexity of prompt engineering, and creates a playground for users to explore different variables and iterate on their ideas. We discuss the implications of our findings for UI-centric interactive paradigm in code generation LLMs.")],-1),te=e("h4",{id:"learn-from-failure-fine-tuning-llms-with-trial-and-error-data-for-intuitionistic-propositional-logic-proving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learn-from-failure-fine-tuning-llms-with-trial-and-error-data-for-intuitionistic-propositional-logic-proving"},[e("span",null,"Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving")])],-1),ne=e("p",null,[e("strong",null,"Authors"),t(": Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang")],-1),ae=e("strong",null,"Link",-1),ie={href:"http://arxiv.org/abs/2404.07382v1",target:"_blank",rel:"noopener noreferrer"},oe=e("p",null,[e("strong",null,"Abstract"),t(": Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.")],-1),se=e("h4",{id:"llms-in-biomedicine-a-study-on-clinical-named-entity-recognition",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-in-biomedicine-a-study-on-clinical-named-entity-recognition"},[e("span",null,"LLMs in Biomedicine: A study on clinical Named Entity Recognition")])],-1),re=e("p",null,[e("strong",null,"Authors"),t(": Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang")],-1),le=e("strong",null,"Link",-1),ce={href:"http://arxiv.org/abs/2404.07376v1",target:"_blank",rel:"noopener noreferrer"},he=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the medical domain by exploring strategies to enhance their performance for the Named-Entity Recognition (NER) task. Specifically, our study reveals the importance of meticulously designed prompts in biomedicine. Strategic selection of in-context examples yields a notable improvement, showcasing ~15-20% increase in F1 score across all benchmark datasets for few-shot clinical NER. Additionally, our findings suggest that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER. Leveraging a medical knowledge base, our proposed method inspired by Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for zero-shot clinical NER. We will release the code upon publication.")],-1),de=e("h4",{id:"from-model-centered-to-human-centered-revision-distance-as-a-metric-for-text-evaluation-in-llms-based-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#from-model-centered-to-human-centered-revision-distance-as-a-metric-for-text-evaluation-in-llms-based-applications"},[e("span",null,"From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications")])],-1),ue=e("p",null,[e("strong",null,"Authors"),t(": Yongqiang Ma, Lizhi Qing, Jiawei Liu, Yangyang Kang, Yue Zhang, Wei Lu, Xiaozhong Liu, Qikai Cheng")],-1),ge=e("strong",null,"Link",-1),pe={href:"http://arxiv.org/abs/2404.07108v2",target:"_blank",rel:"noopener noreferrer"},me=e("p",null,[e("strong",null,"Abstract"),t(": Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed "),e("code",null,"Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, "),t("Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.")],-1),fe=e("h4",{id:"metacheckgpt-a-multi-task-hallucination-detector-using-llm-uncertainty-and-meta-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#metacheckgpt-a-multi-task-hallucination-detector-using-llm-uncertainty-and-meta-models"},[e("span",null,"MetaCheckGPT -- A Multi-task Hallucination Detector Using LLM Uncertainty and Meta-models")])],-1),ve=e("p",null,[e("strong",null,"Authors"),t(": Rahul Mehta, Andrew Hoblitzell, Jack O'Keefe, Hyeju Jang, Vasudeva Varma")],-1),be=e("strong",null,"Link",-1),ye={href:"http://arxiv.org/abs/2404.06948v2",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,[e("strong",null,"Abstract"),t(": Hallucinations in large language models (LLMs) have recently become a significant problem. A recent effort in this direction is a shared task at Semeval 2024 Task 6, SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. This paper describes our winning solution ranked 1st and 2nd in the 2 sub-tasks of model agnostic and model aware tracks respectively. We propose a meta-regressor framework of LLMs for model evaluation and integration that achieves the highest scores on the leaderboard. We also experiment with various transformer-based models and black box methods like ChatGPT, Vectara, and others. In addition, we perform an error analysis comparing GPT4 against our best model which shows the limitations of the former.")],-1),we=e("h4",{id:"goex-perspectives-and-designs-towards-a-runtime-for-autonomous-llm-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#goex-perspectives-and-designs-towards-a-runtime-for-autonomous-llm-applications"},[e("span",null,"GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications")])],-1),_e=e("p",null,[e("strong",null,"Authors"),t(": Shishir G. Patil, Tianjun Zhang, Vivian Fang, Noppapon C., Roy Huang, Aaron Hao, Martin Casado, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica")],-1),ke=e("strong",null,"Link",-1),xe={href:"http://arxiv.org/abs/2404.06921v1",target:"_blank",rel:"noopener noreferrer"},Me=e("p",null,[e("strong",null,"Abstract"),t(': Large Language Models (LLMs) are evolving beyond their classical role of providing information within dialogue systems to actively engaging with tools and performing actions on real-world applications and services. Today, humans verify the correctness and appropriateness of the LLM-generated outputs (e.g., code, functions, or actions) before putting them into real-world execution. This poses significant challenges as code comprehension is well known to be notoriously difficult. In this paper, we study how humans can efficiently collaborate with, delegate to, and supervise autonomous LLMs in the future. We argue that in many cases, "post-facto validation" - verifying the correctness of a proposed action after seeing the output - is much easier than the aforementioned "pre-facto validation" setting. The core concept behind enabling a post-facto validation system is the integration of an intuitive undo feature, and establishing a damage confinement for the LLM-generated actions as effective strategies to mitigate the associated risks. Using this, a human can now either revert the effect of an LLM-generated output or be confident that the potential risk is bounded. We believe this is critical to unlock the potential for LLM agents to interact with applications and services with limited (post-facto) human involvement. We describe the design and implementation of our open-source runtime for executing LLM actions, Gorilla Execution Engine (GoEX), and present open research questions towards realizing the goal of LLMs and applications interacting with each other with minimal human supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.')],-1),Ae=e("h4",{id:"simpler-becomes-harder-do-llms-exhibit-a-coherent-behavior-on-simplified-corpora",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#simpler-becomes-harder-do-llms-exhibit-a-coherent-behavior-on-simplified-corpora"},[e("span",null,"Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on Simplified Corpora?")])],-1),Te=e("p",null,[e("strong",null,"Authors"),t(": Miriam Anschütz, Edoardo Mosca, Georg Groh")],-1),Ce=e("strong",null,"Link",-1),Se={href:"http://arxiv.org/abs/2404.06838v1",target:"_blank",rel:"noopener noreferrer"},Ie=e("p",null,[e("strong",null,"Abstract"),t(": Text simplification seeks to improve readability while retaining the original content and meaning. Our study investigates whether pre-trained classifiers also maintain such coherence by comparing their predictions on both original and simplified inputs. We conduct experiments using 11 pre-trained models, including BERT and OpenAI's GPT 3.5, across six datasets spanning three languages. Additionally, we conduct a detailed analysis of the correlation between prediction change rates and simplification types/strengths. Our findings reveal alarming inconsistencies across all languages and models. If not promptly addressed, simplified inputs can be easily exploited to craft zero-iteration model-agnostic adversarial attacks with success rates of up to 50%")],-1),ze=e("h4",{id:"does-mapo-tofu-contain-coffee-probing-llms-for-food-related-cultural-knowledge",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#does-mapo-tofu-contain-coffee-probing-llms-for-food-related-cultural-knowledge"},[e("span",null,"Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge")])],-1),Ee=e("p",null,[e("strong",null,"Authors"),t(": Li Zhou, Taelin Karidi, Nicolas Garneau, Yong Cao, Wanlong Liu, Wenyu Chen, Daniel Hershcovich")],-1),qe=e("strong",null,"Link",-1),Pe={href:"http://arxiv.org/abs/2404.06833v1",target:"_blank",rel:"noopener noreferrer"},We=e("p",null,[e("strong",null,"Abstract"),t(": Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains.")],-1),Ge=e("h4",{id:"not-all-contexts-are-equal-teaching-llms-credibility-aware-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#not-all-contexts-are-equal-teaching-llms-credibility-aware-generation"},[e("span",null,"Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation")])],-1),Re=e("p",null,[e("strong",null,"Authors"),t(": Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun")],-1),He=e("strong",null,"Link",-1),Be={href:"http://arxiv.org/abs/2404.06809v1",target:"_blank",rel:"noopener noreferrer"},De=e("p",null,[e("strong",null,"Abstract"),t(": The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models' capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and utilize credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit resilience against the disruption caused by noisy documents, thereby maintaining robust performance. Moreover, our model supports customized credibility, offering a wide range of potential applications.")],-1),Oe=e("h4",{id:"mathvc-an-llm-simulated-multi-character-virtual-classroom-for-mathematics-education",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mathvc-an-llm-simulated-multi-character-virtual-classroom-for-mathematics-education"},[e("span",null,"MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education")])],-1),Fe=e("p",null,[e("strong",null,"Authors"),t(": Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, Ziyu Yao")],-1),je=e("strong",null,"Link",-1),Ke={href:"http://arxiv.org/abs/2404.06711v1",target:"_blank",rel:"noopener noreferrer"},Ye=e("p",null,[e("strong",null,"Abstract"),t(`: Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, large language models (LLMs) have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of LLMs, in this work, we present MATHVC, the very first LLM-powered virtual classroom containing multiple LLM-simulated student characters, with whom a human student can practice their MM skill. To encourage each LLM character's behaviors to be aligned with their specified math-relevant properties (termed "characteristics alignment") and the overall conversational procedure to be close to an authentic student MM discussion (termed "conversational procedural alignment"), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our simulation approach and showed the promise for MATHVC to benefit real-life students in the future.`)],-1),Ne=e("h4",{id:"culturalteaming-ai-assisted-interactive-red-teaming-for-challenging-llms-lack-of-multicultural-knowledge",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#culturalteaming-ai-assisted-interactive-red-teaming-for-challenging-llms-lack-of-multicultural-knowledge"},[e("span",null,"CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge")])],-1),Ze=e("p",null,[e("strong",null,"Authors"),t(": Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, Yejin Choi")],-1),Je=e("strong",null,"Link",-1),Ve={href:"http://arxiv.org/abs/2404.06664v1",target:"_blank",rel:"noopener noreferrer"},Ue=e("p",null,[e("strong",null,"Abstract"),t(": Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks. Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences. Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency.")],-1),Qe=e("h2",{id:"_2024-04-09",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-09"},[e("span",null,"2024-04-09")])],-1),Xe=e("h4",{id:"khayyam-challenge-persianmmlu-is-your-llm-truly-wise-to-the-persian-language",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#khayyam-challenge-persianmmlu-is-your-llm-truly-wise-to-the-persian-language"},[e("span",null,"Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?")])],-1),$e=e("p",null,[e("strong",null,"Authors"),t(": Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban")],-1),et=e("strong",null,"Link",-1),tt={href:"http://arxiv.org/abs/2404.06644v1",target:"_blank",rel:"noopener noreferrer"},nt=e("p",null,[e("strong",null,"Abstract"),t(": Evaluating Large Language Models (LLMs) is challenging due to their generative nature, necessitating precise evaluation methodologies. Additionally, non-English LLM evaluation lags behind English, resulting in the absence or weakness of LLMs for many languages. In response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,192 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages. The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of LLMs that support the Persian language. Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc., aimed at assessing different facets of LLMs such as language comprehension, reasoning, and information retrieval across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort. Previous works lacked an evaluation framework that combined all of these features into a single comprehensive benchmark. Furthermore, we evaluate a wide range of existing LLMs that support the Persian language, with statistical analyses and interpretations of their outputs.")],-1),at=e("h4",{id:"sandwich-attack-multi-language-mixture-adaptive-attack-on-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sandwich-attack-multi-language-mixture-adaptive-attack-on-llms"},[e("span",null,"Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs")])],-1),it=e("p",null,[e("strong",null,"Authors"),t(": Bibek Upadhayay, Vahid Behzadan")],-1),ot=e("strong",null,"Link",-1),st={href:"http://arxiv.org/abs/2404.07242v1",target:"_blank",rel:"noopener noreferrer"},rt=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) are increasingly being developed and applied, but their widespread use faces challenges. These include aligning LLMs' responses with human values to prevent harmful outputs, which is addressed through safety training methods. Even so, bad actors and malicious users have succeeded in attempts to manipulate the LLMs to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights. Another challenge is the multilingual capabilities of LLMs, which enable the model to understand and respond in multiple languages. Consequently, attackers exploit the unbalanced pre-training datasets of LLMs in different languages and the comparatively lower model performance in low-resource languages than high-resource ones. As a result, attackers use a low-resource languages to intentionally manipulate the model to create harmful responses. Many of the similar attack vectors have been patched by model providers, making the LLMs more robust against language-based manipulation. In this paper, we introduce a new black-box attack vector called the \\emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art LLMs into generating harmful and misaligned responses. Our experiments with five different models, namely Google's Bard, Gemini Pro, LLaMA-2-70-B-Chat, GPT-3.5-Turbo, GPT-4, and Claude-3-OPUS, show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient LLMs, ensuring they serve the public good while minimizing potential for misuse.")],-1),lt=e("h4",{id:"comparing-two-model-designs-for-clinical-note-generation-is-an-llm-a-useful-evaluator-of-consistency",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#comparing-two-model-designs-for-clinical-note-generation-is-an-llm-a-useful-evaluator-of-consistency"},[e("span",null,"Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?")])],-1),ct=e("p",null,[e("strong",null,"Authors"),t(": Nathan Brake, Thomas Schaaf")],-1),ht=e("strong",null,"Link",-1),dt={href:"http://arxiv.org/abs/2404.06503v1",target:"_blank",rel:"noopener noreferrer"},ut=e("p",null,[e("strong",null,"Abstract"),t(": Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.")],-1),gt=e("h4",{id:"pitfalls-of-conversational-llms-on-news-debiasing",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pitfalls-of-conversational-llms-on-news-debiasing"},[e("span",null,"Pitfalls of Conversational LLMs on News Debiasing")])],-1),pt=e("p",null,[e("strong",null,"Authors"),t(": Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek")],-1),mt=e("strong",null,"Link",-1),ft={href:"http://arxiv.org/abs/2404.06488v1",target:"_blank",rel:"noopener noreferrer"},vt=e("p",null,[e("strong",null,"Abstract"),t(": This paper addresses debiasing in news editing and evaluates the effectiveness of conversational Large Language Models in this task. We designed an evaluation checklist tailored to news editors' perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT, introduced unnecessary changes that may impact the author's style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.")],-1),bt=e("h4",{id:"ada-leval-evaluating-long-context-llms-with-length-adaptable-benchmarks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ada-leval-evaluating-long-context-llms-with-length-adaptable-benchmarks"},[e("span",null,"Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")])],-1),yt=e("p",null,[e("strong",null,"Authors"),t(": Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen")],-1),Lt=e("strong",null,"Link",-1),wt={href:"http://arxiv.org/abs/2404.06480v1",target:"_blank",rel:"noopener noreferrer"},_t=e("p",null,[e("strong",null,"Abstract"),t(": Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.")],-1),kt=e("h4",{id:"agentquest-a-modular-benchmark-framework-to-measure-progress-and-improve-llm-agents",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#agentquest-a-modular-benchmark-framework-to-measure-progress-and-improve-llm-agents"},[e("span",null,"AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents")])],-1),xt=e("p",null,[e("strong",null,"Authors"),t(": Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence")],-1),Mt=e("strong",null,"Link",-1),At={href:"http://arxiv.org/abs/2404.06411v1",target:"_blank",rel:"noopener noreferrer"},Tt=e("p",null,[e("strong",null,"Abstract"),t(": The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.")],-1),Ct=e("h4",{id:"model-generation-from-requirements-with-llms-an-exploratory-study",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#model-generation-from-requirements-with-llms-an-exploratory-study"},[e("span",null,"Model Generation from Requirements with LLMs: an Exploratory Study")])],-1),St=e("p",null,[e("strong",null,"Authors"),t(": Alessio Ferrari, Sallam Abualhaija, Chetan Arora")],-1),It=e("strong",null,"Link",-1),zt={href:"http://arxiv.org/abs/2404.06371v1",target:"_blank",rel:"noopener noreferrer"},Et=e("p",null,[e("strong",null,"Abstract"),t(": Complementing natural language (NL) requirements with graphical models can improve stakeholders' communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative large language models (LLMs), ChatGPT being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of ChatGPT to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by ChatGPT for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of LLMs in the RE process, and open the door to novel RE-specific prompting strategies targeting effective model generation.")],-1),qt=e("h4",{id:"llms-reading-comprehension-is-affected-by-parametric-knowledge-and-struggles-with-hypothetical-statements",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-reading-comprehension-is-affected-by-parametric-knowledge-and-struggles-with-hypothetical-statements"},[e("span",null,"LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements")])],-1),Pt=e("p",null,[e("strong",null,"Authors"),t(": Victoria Basmov, Yoav Goldberg, Reut Tsarfaty")],-1),Wt=e("strong",null,"Link",-1),Gt={href:"http://arxiv.org/abs/2404.06283v1",target:"_blank",rel:"noopener noreferrer"},Rt=e("p",null,[e("strong",null,"Abstract"),t(": The task of reading comprehension (RC), often implemented as context-based question answering (QA), provides a primary means to assess language models' natural language understanding (NLU) capabilities. Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive. If the context aligns with the LLMs' internal knowledge, it is hard to discern whether the models' answers stem from context comprehension or from LLMs' internal information. Conversely, using data that conflicts with the models' knowledge creates erroneous trends which distort the results. To address this issue, we suggest to use RC on imaginary data, based on fictitious facts and entities. This task is entirely independent of the models' world knowledge, enabling us to evaluate LLMs' linguistic abilities without the interference of parametric knowledge. Testing ChatGPT, GPT-4, LLaMA 2 and Mixtral on such imaginary data, we uncover a class of linguistic phenomena posing a challenge to current LLMs, involving thinking in terms of alternative, hypothetical scenarios. While all the models handle simple affirmative and negative contexts with high accuracy, they are much more prone to error when dealing with modal and conditional contexts. Crucially, these phenomena also trigger the LLMs' vulnerability to knowledge-conflicts again. In particular, while some models prove virtually unaffected by knowledge conflicts in affirmative and negative contexts, when faced with more semantically involved modal and conditional environments, they often fail to separate the text from their internal knowledge.")],-1),Ht=e("h4",{id:"a-rag-method-for-source-code-inquiry-tailored-to-long-context-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-rag-method-for-source-code-inquiry-tailored-to-long-context-llms"},[e("span",null,"A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs")])],-1),Bt=e("p",null,[e("strong",null,"Authors"),t(": Toshihiro Kamiya")],-1),Dt=e("strong",null,"Link",-1),Ot={href:"http://arxiv.org/abs/2404.06082v1",target:"_blank",rel:"noopener noreferrer"},Ft=e("p",null,[e("strong",null,"Abstract"),t(": Although the context length limitation of large language models (LLMs) has been mitigated, it still hinders their application to software development tasks. This study proposes a method incorporating execution traces into RAG for inquiries about source code. Small-scale experiments confirm a tendency for the method to contribute to improving LLM response quality.")],-1),jt=e("h4",{id:"on-evaluating-the-efficiency-of-source-code-generated-by-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#on-evaluating-the-efficiency-of-source-code-generated-by-llms"},[e("span",null,"On Evaluating the Efficiency of Source Code Generated by LLMs")])],-1),Kt=e("p",null,[e("strong",null,"Authors"),t(": Changan Niu, Ting Zhang, Chuanyi Li, Bin Luo, Vincent Ng")],-1),Yt=e("strong",null,"Link",-1),Nt={href:"http://arxiv.org/abs/2404.06041v1",target:"_blank",rel:"noopener noreferrer"},Zt=e("p",null,[e("strong",null,"Abstract"),t(": Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.")],-1),Jt=e("h4",{id:"pm4py-llm-a-comprehensive-module-for-implementing-pm-on-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pm4py-llm-a-comprehensive-module-for-implementing-pm-on-llms"},[e("span",null,"PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs")])],-1),Vt=e("p",null,[e("strong",null,"Authors"),t(": Alessandro Berti")],-1),Ut=e("strong",null,"Link",-1),Qt={href:"http://arxiv.org/abs/2404.06035v1",target:"_blank",rel:"noopener noreferrer"},Xt=e("p",null,[e("strong",null,"Abstract"),t(": pm4py is a process mining library for Python implementing several process mining (PM) artifacts and algorithms. It also offers methods to integrate PM with large language models (LLMs). This paper examines how the current paradigms of PM on LLM are implemented in pm4py, identifying challenges such as privacy, hallucinations, and the context window limit.")],-1),$t=e("h4",{id:"aegis-online-adaptive-ai-content-safety-moderation-with-ensemble-of-llm-experts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aegis-online-adaptive-ai-content-safety-moderation-with-ensemble-of-llm-experts"},[e("span",null,"AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts")])],-1),en=e("p",null,[e("strong",null,"Authors"),t(": Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien")],-1),tn=e("strong",null,"Link",-1),nn={href:"http://arxiv.org/abs/2404.05993v1",target:"_blank",rel:"noopener noreferrer"},an=e("p",null,[e("strong",null,"Abstract"),t(": As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment")],-1),on=e("h4",{id:"visualwebbench-how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#visualwebbench-how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding"},[e("span",null,"VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?")])],-1),sn=e("p",null,[e("strong",null,"Authors"),t(": Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue")],-1),rn=e("strong",null,"Link",-1),ln={href:"http://arxiv.org/abs/2404.05955v1",target:"_blank",rel:"noopener noreferrer"},cn=e("p",null,[e("strong",null,"Abstract"),t(": Multimodal Large Language models (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive benchmarks. Existing benchmarks are either designed for general multimodal tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as OCR, understanding, and grounding. In this paper, we introduce \\bench{}, a multimodal benchmark designed to assess the capabilities of MLLMs across a variety of web tasks. \\bench{} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3 series, and GPT-4V(ision) on \\bench{}, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate grounding in text-rich environments and subpar performance with low-resolution image inputs. We believe \\bench{} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications.")],-1),hn=e("h2",{id:"_2024-04-08",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-08"},[e("span",null,"2024-04-08")])],-1),dn=e("h4",{id:"llm-augmented-retrieval-enhancing-retrieval-models-through-language-models-and-doc-level-embedding",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-augmented-retrieval-enhancing-retrieval-models-through-language-models-and-doc-level-embedding"},[e("span",null,"LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding")])],-1),un=e("p",null,[e("strong",null,"Authors"),t(": Mingrui Wu, Sheng Cao")],-1),gn=e("strong",null,"Link",-1),pn={href:"http://arxiv.org/abs/2404.05825v1",target:"_blank",rel:"noopener noreferrer"},mn=e("p",null,[e("strong",null,"Abstract"),t(": Recently embedding-based retrieval or dense retrieval have shown state of the art results, compared with traditional sparse or bag-of-words based approaches. This paper introduces a model-agnostic doc-level embedding framework through large language model (LLM) augmentation. In addition, it also improves some important components in the retrieval model training process, such as negative sampling, loss function, etc. By implementing this LLM-augmented retrieval framework, we have been able to significantly improve the effectiveness of widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2), thereby achieving state-of-the-art results on LoTTE datasets and BEIR datasets.")],-1),fn=e("h4",{id:"ferret-ui-grounded-mobile-ui-understanding-with-multimodal-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ferret-ui-grounded-mobile-ui-understanding-with-multimodal-llms"},[e("span",null,"Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs")])],-1),vn=e("p",null,[e("strong",null,"Authors"),t(": Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan")],-1),bn=e("strong",null,"Link",-1),yn={href:"http://arxiv.org/abs/2404.05719v1",target:"_blank",rel:"noopener noreferrer"},Ln=e("p",null,[e("strong",null,"Abstract"),t(`: Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate "any resolution" on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens). Both sub-images are encoded separately before being sent to LLMs. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.`)],-1),wn=e("h4",{id:"moma-multimodal-llm-adapter-for-fast-personalized-image-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#moma-multimodal-llm-adapter-for-fast-personalized-image-generation"},[e("span",null,"MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation")])],-1),_n=e("p",null,[e("strong",null,"Authors"),t(": Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang")],-1),kn=e("strong",null,"Link",-1),xn={href:"http://arxiv.org/abs/2404.05674v1",target:"_blank",rel:"noopener noreferrer"},Mn=e("p",null,[e("strong",null,"Abstract"),t(": In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements.")],-1),An=e("h4",{id:"the-fact-selection-problem-in-llm-based-program-repair",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-fact-selection-problem-in-llm-based-program-repair"},[e("span",null,"The Fact Selection Problem in LLM-Based Program Repair")])],-1),Tn=e("p",null,[e("strong",null,"Authors"),t(": Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, Sergey Mechtaev")],-1),Cn=e("strong",null,"Link",-1),Sn={href:"http://arxiv.org/abs/2404.05520v2",target:"_blank",rel:"noopener noreferrer"},In=e("p",null,[e("strong",null,"Abstract"),t(": Recent research has shown that incorporating bug-related facts, such as stack traces and GitHub issues, into prompts enhances the bug-fixing capabilities of large language models (LLMs). Considering the ever-increasing context window of these models, a critical question arises: what and how many facts should be included in prompts to maximise the chance of correctly fixing bugs? To answer this question, we conducted a large-scale study, employing over 19K prompts featuring various combinations of seven diverse facts to rectify 314 bugs from open-source Python projects within the BugsInPy benchmark. Our findings revealed that each fact, ranging from simple syntactic details like code context to semantic information previously unexplored in the context of LLMs such as angelic values, is beneficial. Specifically, each fact aids in fixing some bugs that would remain unresolved or only be fixed with a low success rate without it. Importantly, we discovered that the effectiveness of program repair prompts is non-monotonic over the number of used facts; using too many facts leads to subpar outcomes. These insights led us to define the fact selection problem: determining the optimal set of facts for inclusion in a prompt to maximise LLM's performance on a given task instance. We found that there is no one-size-fits-all set of facts for bug repair. Therefore, we developed a basic statistical model, named Maniple, which selects facts specific to a given bug to include in the prompt. This model significantly surpasses the performance of the best generic fact set. To underscore the significance of the fact selection problem, we benchmarked Maniple against the state-of-the-art zero-shot, non-conversational LLM-based bug repair methods. On our testing dataset of 157 bugs, Maniple repairs 88 bugs, 17% above the best configuration.")],-1),zn=e("h4",{id:"petkaz-at-semeval-2024-task-3-advancing-emotion-classification-with-an-llm-for-emotion-cause-pair-extraction-in-conversations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#petkaz-at-semeval-2024-task-3-advancing-emotion-classification-with-an-llm-for-emotion-cause-pair-extraction-in-conversations"},[e("span",null,"PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM for Emotion-Cause Pair Extraction in Conversations")])],-1),En=e("p",null,[e("strong",null,"Authors"),t(": Roman Kazakov, Kseniia Petukhova, Ekaterina Kochmar")],-1),qn=e("strong",null,"Link",-1),Pn={href:"http://arxiv.org/abs/2404.05502v1",target:"_blank",rel:"noopener noreferrer"},Wn=e("p",null,[e("strong",null,"Abstract"),t(': In this paper, we present our submission to the SemEval-2023 Task~3 "The Competition of Multimodal Emotion Cause Analysis in Conversations", focusing on extracting emotion-cause pairs from dialogs. Specifically, our approach relies on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based neural network to detect causes. We score 2nd in the ranking for Subtask 1, demonstrating the effectiveness of our approach through one of the highest weighted-average proportional F1 scores recorded at 0.264.')],-1),Gn=e("h4",{id:"petkaz-at-semeval-2024-task-8-can-linguistics-capture-the-specifics-of-llm-generated-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#petkaz-at-semeval-2024-task-8-can-linguistics-capture-the-specifics-of-llm-generated-text"},[e("span",null,"PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of LLM-generated Text?")])],-1),Rn=e("p",null,[e("strong",null,"Authors"),t(": Kseniia Petukhova, Roman Kazakov, Ekaterina Kochmar")],-1),Hn=e("strong",null,"Link",-1),Bn={href:"http://arxiv.org/abs/2404.05483v1",target:"_blank",rel:"noopener noreferrer"},Dn=e("p",null,[e("strong",null,"Abstract"),t(': In this paper, we present our submission to the SemEval-2024 Task 8 "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection", focusing on the detection of machine-generated texts (MGTs) in English. Specifically, our approach relies on combining embeddings from the RoBERTa-base with diversity features and uses a resampled training set. We score 12th from 124 in the ranking for Subtask A (monolingual track), and our results show that our approach is generalizable across unseen models and domains, achieving an accuracy of 0.91.')],-1),On=e("h4",{id:"llm-reasoners-new-evaluation-library-and-analysis-of-step-by-step-reasoning-with-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-reasoners-new-evaluation-library-and-analysis-of-step-by-step-reasoning-with-large-language-models"},[e("span",null,"LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models")])],-1),Fn=e("p",null,[e("strong",null,"Authors"),t(": Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu")],-1),jn=e("strong",null,"Link",-1),Kn={href:"http://arxiv.org/abs/2404.05221v1",target:"_blank",rel:"noopener noreferrer"},Yn=e("p",null,[e("strong",null,"Abstract"),t(": Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.")],-1),Nn=e("h4",{id:"evaluation-of-an-llm-in-identifying-logical-fallacies-a-call-for-rigor-when-adopting-llms-in-hci-research",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluation-of-an-llm-in-identifying-logical-fallacies-a-call-for-rigor-when-adopting-llms-in-hci-research"},[e("span",null,"Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor When Adopting LLMs in HCI Research")])],-1),Zn=e("p",null,[e("strong",null,"Authors"),t(": Gionnieve Lim, Simon T. Perrault")],-1),Jn=e("strong",null,"Link",-1),Vn={href:"http://arxiv.org/abs/2404.05213v1",target:"_blank",rel:"noopener noreferrer"},Un=e("p",null,[e("strong",null,"Abstract"),t(": There is increasing interest in the adoption of LLMs in HCI research. However, LLMs may often be regarded as a panacea because of their powerful capabilities with an accompanying oversight on whether they are suitable for their intended tasks. We contend that LLMs should be adopted in a critical manner following rigorous evaluation. Accordingly, we present the evaluation of an LLM in identifying logical fallacies that will form part of a digital misinformation intervention. By comparing to a labeled dataset, we found that GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes invalid or unidentified instances, an accuracy of 0.90. This gives us the confidence to proceed with the application of the LLM while keeping in mind the areas where it still falls short. The paper describes our evaluation approach, results and reflections on the use of the LLM for our intended task.")],-1),Qn=e("h4",{id:"progressive-alignment-with-vlm-llm-feature-to-augment-defect-classification-for-the-ase-dataset",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#progressive-alignment-with-vlm-llm-feature-to-augment-defect-classification-for-the-ase-dataset"},[e("span",null,"Progressive Alignment with VLM-LLM Feature to Augment Defect Classification for the ASE Dataset")])],-1),Xn=e("p",null,[e("strong",null,"Authors"),t(": Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu")],-1),$n=e("strong",null,"Link",-1),ea={href:"http://arxiv.org/abs/2404.05183v1",target:"_blank",rel:"noopener noreferrer"},ta=e("p",null,[e("strong",null,"Abstract"),t(`: Traditional defect classification approaches are facing with two barriers. (1) Insufficient training data and unstable data quality. Collecting sufficient defective sample is expensive and time-costing, consequently leading to dataset variance. It introduces the difficulty on recognition and learning. (2) Over-dependence on visual modality. When the image pattern and texture is monotonic for all defect classes in a given dataset, the performance of conventional AOI system cannot be guaranteed. In scenarios where image quality is compromised due to mechanical failures or when defect information is inherently difficult to discern, the performance of deep models cannot be guaranteed. A main question is, "how to solve those two problems when they occur at the same time?" The feasible strategy is to explore another feature within dataset and combine an eminent vision-language model (VLM) and Large-Language model (LLM) with their astonishing zero-shot capability. In this work, we propose the special ASE dataset, including rich data description recorded on image, for defect classification, but the defect feature is uneasy to learn directly. Secondly, We present the prompting for VLM-LLM against defect classification with the proposed ASE dataset to activate extra-modality feature from images to enhance performance. Then, We design the novel progressive feature alignment (PFA) block to refine image-text feature to alleviate the difficulty of alignment under few-shot scenario. Finally, the proposed Cross-modality attention fusion (CMAF) module can effectively fuse different modality feature. Experiment results have demonstrated our method's effectiveness over several defect classification methods for the ASE dataset.`)],-1),na=e("h4",{id:"enhancing-clinical-efficiency-through-llm-discharge-note-generation-for-cardiac-patients",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-clinical-efficiency-through-llm-discharge-note-generation-for-cardiac-patients"},[e("span",null,"Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac Patients")])],-1),aa=e("p",null,[e("strong",null,"Authors"),t(": HyoJe Jung, Yunha Kim, Heejung Choi, Hyeram Seo, Minkyoung Kim, JiYe Han, Gaeun Kee, Seohyun Park, Soyoung Ko, Byeolhee Kim, Suyeon Kim, Tae Joon Jun, Young-Hak Kim")],-1),ia=e("strong",null,"Link",-1),oa={href:"http://arxiv.org/abs/2404.05144v1",target:"_blank",rel:"noopener noreferrer"},sa=e("p",null,[e("strong",null,"Abstract"),t(": Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication. However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors. The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in healthcare. This study directly addresses the inefficiencies and inaccuracies in creating discharge notes manually, particularly for cardiac patients, by employing AI techniques, specifically large language model (LLM). Utilizing a substantial dataset from a cardiology center, encompassing wide-ranging medical records and physician assessments, our research evaluates the capability of LLM to enhance the documentation process. Among the various models assessed, Mistral-7B distinguished itself by accurately generating discharge notes that significantly improve both documentation efficiency and the continuity of care for patients. These notes underwent rigorous qualitative evaluation by medical expert, receiving high marks for their clinical relevance, completeness, readability, and contribution to informed decision-making and care planning. Coupled with quantitative analyses, these results confirm Mistral-7B's efficacy in distilling complex medical information into concise, coherent summaries. Overall, our findings illuminate the considerable promise of specialized LLM, such as Mistral-7B, in refining healthcare documentation workflows and advancing patient care. This study lays the groundwork for further integrating advanced AI technologies in healthcare, demonstrating their potential to revolutionize patient documentation and support better care outcomes.")],-1),ra=e("h2",{id:"_2024-04-07",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-07"},[e("span",null,"2024-04-07")])],-1),la=e("h4",{id:"clinical-trials-protocol-authoring-using-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#clinical-trials-protocol-authoring-using-llms"},[e("span",null,"Clinical Trials Protocol Authoring using LLMs")])],-1),ca=e("p",null,[e("strong",null,"Authors"),t(": Morteza Maleki")],-1),ha=e("strong",null,"Link",-1),da={href:"http://arxiv.org/abs/2404.05044v1",target:"_blank",rel:"noopener noreferrer"},ua=e("p",null,[e("strong",null,"Abstract"),t(": This report embarks on a mission to revolutionize clinical trial protocol development through the integration of advanced AI technologies. With a focus on leveraging the capabilities of generative AI, specifically GPT-4, this initiative aimed to streamline and enhance the efficiency and accuracy of clinical trial protocols. The methodology encompassed a detailed analysis and preparation of comprehensive drug and study level metadata, followed by the deployment of GPT-4 for automated protocol section generation. Results demonstrated a significant improvement in protocol authoring, highlighted by increases in efficiency, accuracy, and the customization of protocols to specific trial requirements. Challenges encountered during model selection and prompt engineering were systematically addressed, leading to refined methodologies that capitalized on the advanced text generation capabilities of GPT-4. This project not only showcases the practical applications and benefits of generative AI in clinical trial design but also sets a foundation for future innovations in the field.")],-1),ga=e("h4",{id:"adapting-llms-for-efficient-context-processing-through-soft-prompt-compression",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#adapting-llms-for-efficient-context-processing-through-soft-prompt-compression"},[e("span",null,"Adapting LLMs for Efficient Context Processing through Soft Prompt Compression")])],-1),pa=e("p",null,[e("strong",null,"Authors"),t(": Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian Floyd")],-1),ma=e("strong",null,"Link",-1),fa={href:"http://arxiv.org/abs/2404.04997v1",target:"_blank",rel:"noopener noreferrer"},va=e("p",null,[e("strong",null,"Abstract"),t(": The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.")],-1),ba=e("h4",{id:"enhancing-llm-based-test-generation-for-hard-to-cover-branches-via-program-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-llm-based-test-generation-for-hard-to-cover-branches-via-program-analysis"},[e("span",null,"Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis")])],-1),ya=e("p",null,[e("strong",null,"Authors"),t(": Chen Yang, Junjie Chen, Bin Lin, Jianyi Zhou, Ziqi Wang")],-1),La=e("strong",null,"Link",-1),wa={href:"http://arxiv.org/abs/2404.04966v1",target:"_blank",rel:"noopener noreferrer"},_a=e("p",null,[e("strong",null,"Abstract"),t(": Automatic test generation plays a critical role in software quality assurance. While the recent advances in Search-Based Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing test generation techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-to-cover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counter-examples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-of-the-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage.")],-1),ka=e("h4",{id:"ai2apps-a-visual-ide-for-building-llm-based-ai-agent-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ai2apps-a-visual-ide-for-building-llm-based-ai-agent-applications"},[e("span",null,"AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications")])],-1),xa=e("p",null,[e("strong",null,"Authors"),t(": Xin Pang, Zhucong Li, Jiaxiang Chen, Yuan Cheng, Yinghui Xu, Yuan Qi")],-1),Ma=e("strong",null,"Link",-1),Aa={href:"http://arxiv.org/abs/2404.04902v1",target:"_blank",rel:"noopener noreferrer"},Ta=e("p",null,[e("strong",null,"Abstract"),t(": We introduce AI2Apps, a Visual Integrated Development Environment (Visual IDE) with full-cycle capabilities that accelerates developers to build deployable LLM-based AI agent Applications. This Visual IDE prioritizes both the Integrity of its development tools and the Visuality of its components, ensuring a smooth and efficient building experience.On one hand, AI2Apps integrates a comprehensive development toolkit ranging from a prototyping canvas and AI-assisted code editor to agent debugger, management system, and deployment tools all within a web-based graphical user interface. On the other hand, AI2Apps visualizes reusable front-end and back-end code as intuitive drag-and-drop components. Furthermore, a plugin system named AI2Apps Extension (AAE) is designed for Extensibility, showcasing how a new plugin with 20 components enables web agent to mimic human-like browsing behavior. Our case study demonstrates substantial efficiency improvements, with AI2Apps reducing token consumption and API calls when debugging a specific sophisticated multimodal agent by approximately 90% and 80%, respectively. The AI2Apps, including an online demo, open-source code, and a screencast video, is now publicly accessible.")],-1),Ca=e("h4",{id:"prompting-multi-modal-tokens-to-enhance-end-to-end-autonomous-driving-imitation-learning-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#prompting-multi-modal-tokens-to-enhance-end-to-end-autonomous-driving-imitation-learning-with-llms"},[e("span",null,"Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs")])],-1),Sa=e("p",null,[e("strong",null,"Authors"),t(": Yiqun Duan, Qiang Zhang, Renjing Xu")],-1),Ia=e("strong",null,"Link",-1),za={href:"http://arxiv.org/abs/2404.04869v1",target:"_blank",rel:"noopener noreferrer"},Ea=e("p",null,[e("strong",null,"Abstract"),t(": The utilization of Large Language Models (LLMs) within the realm of reinforcement learning, particularly as planners, has garnered a significant degree of attention in recent scholarly literature. However, a substantial proportion of existing research predominantly focuses on planning models for robotics that transmute the outputs derived from perception models into linguistic forms, thus adopting a `pure-language' strategy. In this research, we propose a hybrid End-to-End learning framework for autonomous driving by combining basic driving imitation learning with LLMs based on multi-modality prompt tokens. Instead of simply converting perception results from the separated train model into pure language input, our novelty lies in two aspects. 1) The end-to-end integration of visual and LiDAR sensory input into learnable multi-modality tokens, thereby intrinsically alleviating description bias by separated pre-trained perception models. 2) Instead of directly letting LLMs drive, this paper explores a hybrid setting of letting LLMs help the driving model correct mistakes and complicated scenarios. The results of our experiments suggest that the proposed methodology can attain driving scores of 49.21%, coupled with an impressive route completion rate of 91.34% in the offline evaluation conducted via CARLA. These performance metrics are comparable to the most advanced driving models.")],-1),qa=e("h4",{id:"explaining-eda-synthesis-errors-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#explaining-eda-synthesis-errors-with-llms"},[e("span",null,"Explaining EDA synthesis errors with LLMs")])],-1),Pa=e("p",null,[e("strong",null,"Authors"),t(": Siyu Qiu, Benjamin Tan, Hammond Pearce")],-1),Wa=e("strong",null,"Link",-1),Ga={href:"http://arxiv.org/abs/2404.07235v1",target:"_blank",rel:"noopener noreferrer"},Ra=e("p",null,[e("strong",null,"Abstract"),t(": Training new engineers in digital design is a challenge, particularly when it comes to teaching the complex electronic design automation (EDA) tooling used in this domain. Learners will typically deploy designs in the Verilog and VHDL hardware description languages to Field Programmable Gate Arrays (FPGAs) from Altera (Intel) and Xilinx (AMD) via proprietary closed-source toolchains (Quartus Prime and Vivado, respectively). These tools are complex and difficult to use -- yet, as they are the tools used in industry, they are an essential first step in this space. In this work, we examine how recent advances in artificial intelligence may be leveraged to address aspects of this challenge. Specifically, we investigate if Large Language Models (LLMs), which have demonstrated text comprehension and question-answering capabilities, can be used to generate novice-friendly explanations of compile-time synthesis error messages from Quartus Prime and Vivado. To perform this study we generate 936 error message explanations using three OpenAI LLMs over 21 different buggy code samples. These are then graded for relevance and correctness, and we find that in approximately 71% of cases the LLMs give correct & complete explanations suitable for novice learners.")],-1),Ha=e("h4",{id:"llm-based-multi-agent-systems-for-software-engineering-vision-and-the-road-ahead",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-based-multi-agent-systems-for-software-engineering-vision-and-the-road-ahead"},[e("span",null,"LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead")])],-1),Ba=e("p",null,[e("strong",null,"Authors"),t(": Junda He, Christoph Treude, David Lo")],-1),Da=e("strong",null,"Link",-1),Oa={href:"http://arxiv.org/abs/2404.04834v1",target:"_blank",rel:"noopener noreferrer"},Fa=e("p",null,[e("strong",null,"Abstract"),t(": Integrating Large Language Models(LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities competitive to human planning and reasoning. This paper envisions the evolution of LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted software engineering challenges. LMA systems introduce numerous benefits, including enhanced robustness through collaborative cross-examination, autonomous problem-solving, and scalable solutions to complex software projects. By examining the role of LMA systems in future software engineering practices, this vision paper highlights the potential applications and emerging challenges. We further point to specific opportunities for research and conclude with a research agenda with a set of research questions to guide future research directions.")],-1),ja=e("h4",{id:"low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#low-resource-machine-translation-through-retrieval-augmented-llm-prompting-a-study-on-the-mambai-language"},[e("span",null,"Low-Resource Machine Translation through Retrieval-Augmented LLM Prompting: A Study on the Mambai Language")])],-1),Ka=e("p",null,[e("strong",null,"Authors"),t(": Raphaël Merx, Aso Mahmudi, Katrina Langford, Leo Alberto de Araujo, Ekaterina Vylomova")],-1),Ya=e("strong",null,"Link",-1),Na={href:"http://arxiv.org/abs/2404.04809v1",target:"_blank",rel:"noopener noreferrer"},Za=e("p",null,[e("strong",null,"Abstract"),t(": This study explores the use of large language models (LLMs) for translating English into Mambai, a low-resource Austronesian language spoken in Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, we examine the efficacy of few-shot LLM prompting for machine translation (MT) in this low-resource context. Our methodology involves the strategic selection of parallel sentences and dictionary entries for prompting, aiming to enhance translation accuracy, using open-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find that including dictionary entries in prompts and a mix of sentences retrieved through TF-IDF and semantic embeddings significantly improves translation quality. However, our findings reveal stark disparities in translation performance across test sets, with BLEU scores reaching as high as 21.2 on materials from the language manual, in contrast to a maximum of 4.4 on a test set provided by a native speaker. These results underscore the importance of diverse and representative corpora in assessing MT for low-resource languages. Our research provides insights into few-shot LLM prompting for low-resource MT, and makes available an initial corpus for the Mambai language.")],-1),Ja=e("h4",{id:"squeezeattention-2d-management-of-kv-cache-in-llm-inference-via-layer-wise-optimal-budget",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#squeezeattention-2d-management-of-kv-cache-in-llm-inference-via-layer-wise-optimal-budget"},[e("span",null,"SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget")])],-1),Va=e("p",null,[e("strong",null,"Authors"),t(": Zihao Wang, Shaoduo Gan")],-1),Ua=e("strong",null,"Link",-1),Qa={href:"http://arxiv.org/abs/2404.04793v1",target:"_blank",rel:"noopener noreferrer"},Xa=e("p",null,[e("strong",null,"Abstract"),t(": Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions. Based on our observations regarding layer-wise importance in inference, we propose SqueezeAttention to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative token sparsification algorithms to compress the KV-cache for each layer with its very own budget. By optimizing the KV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves around 30% to 70% of the memory reductions and up to 2.2 times of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention.")],-1),$a=e("h2",{id:"_2024-04-06",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-06"},[e("span",null,"2024-04-06")])],-1),ei=e("h4",{id:"multicalibration-for-confidence-scoring-in-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multicalibration-for-confidence-scoring-in-llms"},[e("span",null,"Multicalibration for Confidence Scoring in LLMs")])],-1),ti=e("p",null,[e("strong",null,"Authors"),t(": Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth")],-1),ni=e("strong",null,"Link",-1),ai={href:"http://arxiv.org/abs/2404.04689v1",target:"_blank",rel:"noopener noreferrer"},ii=e("p",null,[e("strong",null,"Abstract"),t(': This paper proposes the use of "multicalibration" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and "self-annotation" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.')],-1),oi=e("h4",{id:"on-the-limitations-of-large-language-models-llms-false-attribution",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#on-the-limitations-of-large-language-models-llms-false-attribution"},[e("span",null,"On the Limitations of Large Language Models (LLMs): False Attribution")])],-1),si=e("p",null,[e("strong",null,"Authors"),t(": Tosin Adewumi, Nudrat Habib, Lama Alkhaled, Elisa Barney")],-1),ri=e("strong",null,"Link",-1),li={href:"http://arxiv.org/abs/2404.04631v1",target:"_blank",rel:"noopener noreferrer"},ci=e("p",null,[e("strong",null,"Abstract"),t(": In this work, we provide insight into one important limitation of large language models (LLMs), i.e. false attribution, and introduce a new hallucination metric - Simple Hallucination Index (SHI). The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (LLaMA-2-13B, Mixtral 8x7B, and Gemma-7B), especially as human annotation can be costly. We collected the top 10 most popular books, according to Project Gutenberg, divided each one into equal chunks of 400 words, and asked each LLM to predict the author. We then randomly sampled 162 chunks for human evaluation from each of the annotated books, based on the error margin of 7% and a confidence level of 95% for the book with the most chunks (Great Expectations by Charles Dickens, having 922 chunks). The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.737, 0.249, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as an SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which is generalizable to other tasks. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.")],-1),hi=e("h4",{id:"analyzing-llm-usage-in-an-advanced-computing-class-in-india",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#analyzing-llm-usage-in-an-advanced-computing-class-in-india"},[e("span",null,"Analyzing LLM Usage in an Advanced Computing Class in India")])],-1),di=e("p",null,[e("strong",null,"Authors"),t(": Chaitanya Arora, Utkarsh Venaik, Pavit Singh, Sahil Goyal, Jatin Tyagi, Shyama Goel, Ujjwal Singhal, Dhruv Kumar")],-1),ui=e("strong",null,"Link",-1),gi={href:"http://arxiv.org/abs/2404.04603v1",target:"_blank",rel:"noopener noreferrer"},pi=e("p",null,[e("strong",null,"Abstract"),t(": This paper investigates the usage patterns of undergraduate and graduate students when engaging with large language models (LLMs) to tackle programming assignments in the context of advanced computing courses. Existing work predominantly focuses on the influence of LLMs in introductory programming contexts. Additionally, there is a scarcity of studies analyzing actual conversations between students and LLMs. Our study provides a comprehensive quantitative and qualitative analysis of raw interactions between students and LLMs within an advanced computing course (Distributed Systems) at an Indian University. We further complement this by conducting student interviews to gain deeper insights into their usage patterns. Our study shows that students make use of large language models (LLMs) in various ways: generating code or debugging code by identifying and fixing errors. They also copy and paste assignment descriptions into LLM interfaces for specific solutions, ask conceptual questions about complex programming ideas or theoretical concepts, and generate test cases to check code functionality and robustness. Our analysis includes over 4,000 prompts from 411 students and conducting interviews with 10 students. Our analysis shows that LLMs excel at generating boilerplate code and assisting in debugging, while students handle the integration of components and system troubleshooting. This aligns with the learning objectives of advanced computing courses, which are oriented towards teaching students how to build systems and troubleshoot, with less emphasis on generating code from scratch. Therefore, LLM tools can be leveraged to increase student productivity, as shown by the data we collected. This study contributes to the ongoing discussion on LLM use in education, advocating for their usefulness in advanced computing courses to complement higher-level learning and productivity.")],-1),mi=e("h4",{id:"a-map-of-exploring-human-interaction-patterns-with-llm-insights-into-collaboration-and-creativity",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-map-of-exploring-human-interaction-patterns-with-llm-insights-into-collaboration-and-creativity"},[e("span",null,"A Map of Exploring Human Interaction patterns with LLM: Insights into Collaboration and Creativity")])],-1),fi=e("p",null,[e("strong",null,"Authors"),t(": Jiayang Li, Jiale Li")],-1),vi=e("strong",null,"Link",-1),bi={href:"http://arxiv.org/abs/2404.04570v1",target:"_blank",rel:"noopener noreferrer"},yi=e("p",null,[e("strong",null,"Abstract"),t(": The outstanding performance capabilities of large language model have driven the evolution of current AI system interaction patterns. This has led to considerable discussion within the Human-AI Interaction (HAII) community. Numerous studies explore this interaction from technical, design, and empirical perspectives. However, the majority of current literature reviews concentrate on interactions across the wider spectrum of AI, with limited attention given to the specific realm of interaction with LLM. We searched for articles on human interaction with LLM, selecting 110 relevant publications meeting consensus definition of Human-AI interaction. Subsequently, we developed a comprehensive Mapping Procedure, structured in five distinct stages, to systematically analyze and categorize the collected publications. Applying this methodical approach, we meticulously mapped the chosen studies, culminating in a detailed and insightful representation of the research landscape. Overall, our review presents an novel approach, introducing a distinctive mapping method, specifically tailored to evaluate human-LLM interaction patterns. We conducted a comprehensive analysis of the current research in related fields, employing clustering techniques for categorization, which enabled us to clearly delineate the status and challenges prevalent in each identified area.")],-1),Li=e("h4",{id:"iitk-at-semeval-2024-task-2-exploring-the-capabilities-of-llms-for-safe-biomedical-natural-language-inference-for-clinical-trials",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#iitk-at-semeval-2024-task-2-exploring-the-capabilities-of-llms-for-safe-biomedical-natural-language-inference-for-clinical-trials"},[e("span",null,"IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials")])],-1),wi=e("p",null,[e("strong",null,"Authors"),t(": Shreyasi Mandal, Ashutosh Modi")],-1),_i=e("strong",null,"Link",-1),ki={href:"http://arxiv.org/abs/2404.04510v1",target:"_blank",rel:"noopener noreferrer"},xi=e("p",null,[e("strong",null,"Abstract"),t(": Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates LLMs' robustness, consistency, and faithful reasoning when performing Natural Language Inference (NLI) on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains. The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset.")],-1),Mi=e("h2",{id:"_2024-04-05",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-05"},[e("span",null,"2024-04-05")])],-1),Ai=e("h4",{id:"increased-llm-vulnerabilities-from-fine-tuning-and-quantization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#increased-llm-vulnerabilities-from-fine-tuning-and-quantization"},[e("span",null,"Increased LLM Vulnerabilities from Fine-tuning and Quantization")])],-1),Ti=e("p",null,[e("strong",null,"Authors"),t(": Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth Harshangi")],-1),Ci=e("strong",null,"Link",-1),Si={href:"http://arxiv.org/abs/2404.04392v1",target:"_blank",rel:"noopener noreferrer"},Ii=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have become very popular and have found use cases in many domains, such as chatbots, auto-task completion agents, and much more. However, LLMs are vulnerable to different types of attacks, such as jailbreaking, prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo adversarial and alignment training to learn not to generate malicious and toxic content. For specialized use cases, these foundational LLMs are subjected to fine-tuning or quantization for better performance and efficiency. We examine the impact of downstream tasks such as fine-tuning and quantization on LLM vulnerability. We test foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions. Our research shows that fine-tuning and quantization reduces jailbreak resistance significantly, leading to increased LLM vulnerabilities. Finally, we demonstrate the utility of external guardrails in reducing LLM vulnerabilities.")],-1),zi=e("h4",{id:"clickdiffusion-harnessing-llms-for-interactive-precise-image-editing",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#clickdiffusion-harnessing-llms-for-interactive-precise-image-editing"},[e("span",null,"ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing")])],-1),Ei=e("p",null,[e("strong",null,"Authors"),t(": Alec Helbling, Seongmin Lee, Polo Chau")],-1),qi=e("strong",null,"Link",-1),Pi={href:"http://arxiv.org/abs/2404.04376v1",target:"_blank",rel:"noopener noreferrer"},Wi=e("p",null,[e("strong",null,"Abstract"),t(": Recently, researchers have proposed powerful systems for generating and manipulating images using natural language instructions. However, it is difficult to precisely specify many common classes of image transformations with text alone. For example, a user may wish to change the location and breed of a particular dog in an image with several similar dogs. This task is quite difficult with natural language alone, and would require a user to write a laboriously complex prompt that both disambiguates the target dog and describes the destination. We propose ClickDiffusion, a system for precise image manipulation and generation that combines natural language instructions with visual feedback provided by the user through a direct manipulation interface. We demonstrate that by serializing both an image and a multi-modal instruction into a textual representation it is possible to leverage LLMs to perform precise transformations of the layout and appearance of an image. Code available at https://github.com/poloclub/ClickDiffusion.")],-1),Gi=e("h4",{id:"koala-key-frame-conditioned-long-video-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#koala-key-frame-conditioned-long-video-llm"},[e("span",null,"Koala: Key frame-conditioned long video-LLM")])],-1),Ri=e("p",null,[e("strong",null,"Authors"),t(": Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko")],-1),Hi=e("strong",null,"Link",-1),Bi={href:"http://arxiv.org/abs/2404.04346v1",target:"_blank",rel:"noopener noreferrer"},Di=e("p",null,[e("strong",null,"Abstract"),t(": Long video question answering is a challenging task that involves recognizing short-term activities and reasoning about their fine-grained relationships. State-of-the-art video Large Language Models (vLLMs) hold promise as a viable solution due to their demonstrated emergent capabilities on new tasks. However, despite being trained on millions of short seconds-long videos, vLLMs are unable to understand minutes-long videos and accurately answer questions about them. To address this limitation, we propose a lightweight and self-supervised approach, Key frame-conditioned long video-LLM (Koala), that introduces learnable spatiotemporal queries to adapt pretrained vLLMs for generalizing to longer videos. Our approach introduces two new tokenizers that condition on visual tokens computed from sparse video key frames for understanding short and long video moments. We train our proposed approach on HowTo100M and demonstrate its effectiveness on zero-shot long video understanding benchmarks, where it outperforms state-of-the-art large models by 3 - 6% in absolute accuracy across all tasks. Surprisingly, we also empirically show that our approach not only helps a pretrained vLLM to understand long videos but also improves its accuracy on short-term action recognition.")],-1),Oi=e("h4",{id:"chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model"},[e("span",null,"Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model")])],-1),Fi=e("p",null,[e("strong",null,"Authors"),t(": Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang")],-1),ji=e("strong",null,"Link",-1),Ki={href:"http://arxiv.org/abs/2404.04167v1",target:"_blank",rel:"noopener noreferrer"},Yi=e("p",null,[e("strong",null,"Abstract"),t(": In this study, we introduce CT-LLM, a 2B large language model (LLM) that illustrates a pivotal shift towards prioritizing the Chinese language in developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This strategic composition facilitates the model's exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques. Demonstrating remarkable performance on the CHC-Bench, CT-LLM excels in Chinese language tasks, and showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training LLMs predominantly on English corpora and then adapting them to other languages, broadening the horizons for LLM training methodologies. By open-sourcing the full process of training a Chinese LLM, including a detailed data processing procedure with the obtained Massive Appropriate Pretraining Chinese Corpus (MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark (CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster further exploration and innovation in both academia and industry, paving the way for more inclusive and versatile language models.")],-1),Ni=e("h4",{id:"robust-preference-optimization-with-provable-noise-tolerance-for-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robust-preference-optimization-with-provable-noise-tolerance-for-llms"},[e("span",null,"Robust Preference Optimization with Provable Noise Tolerance for LLMs")])],-1),Zi=e("p",null,[e("strong",null,"Authors"),t(": Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye")],-1),Ji=e("strong",null,"Link",-1),Vi={href:"http://arxiv.org/abs/2404.04102v1",target:"_blank",rel:"noopener noreferrer"},Ui=e("p",null,[e("strong",null,"Abstract"),t(": The preference alignment aims to enable large language models (LLMs) to generate responses that conform to human values, which is essential for developing general AI systems. Ranking-based methods -- a promising class of alignment approaches -- learn human preferences from datasets containing response pairs by optimizing the log-likelihood margins between preferred and dis-preferred responses. However, due to the inherent differences in annotators' preferences, ranking labels of comparisons for response pairs are unavoidably noisy. This seriously hurts the reliability of existing ranking-based methods. To address this problem, we propose a provably noise-tolerant preference alignment method, namely RObust Preference Optimization (ROPO). To the best of our knowledge, ROPO is the first preference alignment method with noise-tolerance guarantees. The key idea of ROPO is to dynamically assign conservative gradient weights to response pairs with high label uncertainty, based on the log-likelihood margins between the responses. By effectively suppressing the gradients of noisy samples, our weighting strategy ensures that the expected risk has the same gradient direction independent of the presence and proportion of noise. Experiments on three open-ended text generation tasks with four base models ranging in size from 2.8B to 13B demonstrate that ROPO significantly outperforms existing ranking-based methods.")],-1),Qi=e("h4",{id:"clue-a-clinical-language-understanding-evaluation-for-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#clue-a-clinical-language-understanding-evaluation-for-llms"},[e("span",null,"CLUE: A Clinical Language Understanding Evaluation for LLMs")])],-1),Xi=e("p",null,[e("strong",null,"Authors"),t(": Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek")],-1),$i=e("strong",null,"Link",-1),eo={href:"http://arxiv.org/abs/2404.04067v1",target:"_blank",rel:"noopener noreferrer"},to=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have shown the potential to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical LLMs address healthcare-specific challenges, including privacy demands and computational constraints. However, evaluation of these models has primarily been limited to non-clinical tasks, which do not reflect the complexity of practical clinical applications. Additionally, there has been no thorough comparison between biomedical and general-domain LLMs for clinical tasks. To fill this gap, we present the Clinical Language Understanding Evaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters and four existing tasks designed to test the practical applicability of LLMs in healthcare settings. Our evaluation covers several biomedical and general domain LLMs, providing insights into their clinical performance and applicability. CLUE represents a step towards a standardized approach to evaluating and developing LLMs in healthcare to align future model development with the real-world needs of clinical application. We publish our evaluation and data generation scripts: https://github.com/dadaamin/CLUE")],-1),no=e("h4",{id:"voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots"},[e("span",null,"VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots")])],-1),ao=e("p",null,[e("strong",null,"Authors"),t(": Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson")],-1),io=e("strong",null,"Link",-1),oo={href:"http://arxiv.org/abs/2404.04066v1",target:"_blank",rel:"noopener noreferrer"},so=e("p",null,[e("strong",null,"Abstract"),t(": Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating LLMs as interfaces to robots for high level task planning and code generation have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating LLMs as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using LLMs as speech interfaces for assistive robots. Videos and supporting files are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/")],-1),ro=e("h4",{id:"can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning"},[e("span",null,"Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning")])],-1),lo=e("p",null,[e("strong",null,"Authors"),t(": Gawon Choi, Hyemin Ahn")],-1),co=e("strong",null,"Link",-1),ho={href:"http://arxiv.org/abs/2404.03891v1",target:"_blank",rel:"noopener noreferrer"},uo=e("p",null,[e("strong",null,"Abstract"),t(": In robotics, the use of Large Language Models (LLMs) is becoming prevalent, especially for understanding human commands. In particular, LLMs are utilized as domain-agnostic task planners for high-level human commands. LLMs are capable of Chain-of-Thought (CoT) reasoning, and this allows LLMs to be task planners. However, we need to consider that modern robots still struggle to perform complex actions, and the domains where robots can be deployed are limited in practice. This leads us to pose a question: If small LMs can be trained to reason in chains within a single domain, would even small LMs be good task planners for the robots? To train smaller LMs to reason in chains, we build `COmmand-STeps datasets' (COST) consisting of high-level commands along with corresponding actionable low-level steps, via LLMs. We release not only our datasets but also the prompt templates used to generate them, to allow anyone to build datasets for their domain. We compare GPT3.5 and GPT4 with the finetuned GPT2 for task domains, in tabletop and kitchen environments, and the result shows that GPT2-medium is comparable to GPT3.5 for task planning in a specific domain. Our dataset, code, and more output samples can be found in https://github.com/Gawon-Choi/small-LMs-Task-Planning")],-1),go=e("h4",{id:"extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction"},[e("span",null,"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction")])],-1),po=e("p",null,[e("strong",null,"Authors"),t(": Bowen Zhang, Harold Soh")],-1),mo=e("strong",null,"Link",-1),fo={href:"http://arxiv.org/abs/2404.03868v1",target:"_blank",rel:"noopener noreferrer"},vo=e("p",null,[e("strong",null,"Abstract"),t(": In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schema easily exceed the LLMs' context window length. To address this problem, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.")],-1),bo=e("h2",{id:"_2024-04-04",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-04"},[e("span",null,"2024-04-04")])],-1),yo=e("h4",{id:"cbr-rag-case-based-reasoning-for-retrieval-augmented-generation-in-llms-for-legal-question-answering",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cbr-rag-case-based-reasoning-for-retrieval-augmented-generation-in-llms-for-legal-question-answering"},[e("span",null,"CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering")])],-1),Lo=e("p",null,[e("strong",null,"Authors"),t(": Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, Bruno Fleisch")],-1),wo=e("strong",null,"Link",-1),_o={href:"http://arxiv.org/abs/2404.04302v1",target:"_blank",rel:"noopener noreferrer"},ko=e("p",null,[e("strong",null,"Abstract"),t(": Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.")],-1),xo=e("h4",{id:"self-in-correct-llms-struggle-with-refining-self-generated-responses",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-in-correct-llms-struggle-with-refining-self-generated-responses"},[e("span",null,"SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses")])],-1),Mo=e("p",null,[e("strong",null,"Authors"),t(": Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, Daniel Khashabi")],-1),Ao=e("strong",null,"Link",-1),To={href:"http://arxiv.org/abs/2404.04298v1",target:"_blank",rel:"noopener noreferrer"},Co=e("p",null,[e("strong",null,"Abstract"),t(": Can LLMs continually improve their previous outputs for better results? An affirmative answer would require LLMs to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first introduce a unified framework that allows us to compare the generative and discriminative capability of any model on any task. Then, in our resulting experimental analysis of several LLMs, we do not observe the performance of those models on discrimination to be reliably better than generation. We hope these findings inform the growing literature on self-improvement AI systems.")],-1),So=e("h4",{id:"genqrensemble-zero-shot-llm-ensemble-prompting-for-generative-query-reformulation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#genqrensemble-zero-shot-llm-ensemble-prompting-for-generative-query-reformulation"},[e("span",null,"GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation")])],-1),Io=e("p",null,[e("strong",null,"Authors"),t(": Kaustubh Dhole, Eugene Agichtein")],-1),zo=e("strong",null,"Link",-1),Eo={href:"http://arxiv.org/abs/2404.03746v1",target:"_blank",rel:"noopener noreferrer"},qo=e("p",null,[e("strong",null,"Abstract"),t(": Query Reformulation(QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in large language models. By taking inspiration from the success of ensemble prompting strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.")],-1),Po=e("h4",{id:"fakes-of-varying-shades-how-warning-affects-human-perception-and-engagement-regarding-llm-hallucinations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fakes-of-varying-shades-how-warning-affects-human-perception-and-engagement-regarding-llm-hallucinations"},[e("span",null,"Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations")])],-1),Wo=e("p",null,[e("strong",null,"Authors"),t(": Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee")],-1),Go=e("strong",null,"Link",-1),Ro={href:"http://arxiv.org/abs/2404.03745v1",target:"_blank",rel:"noopener noreferrer"},Ho=e("p",null,[e("strong",null,"Abstract"),t(": The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank content as truthful in the order genuine > minor hallucination > major hallucination and user engagement behaviors mirror this pattern. More importantly, we observed that warning improves hallucination detection without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations.")],-1),Bo=e("h4",{id:"shroom-indelab-at-semeval-2024-task-6-zero-and-few-shot-llm-based-classification-for-hallucination-detection",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#shroom-indelab-at-semeval-2024-task-6-zero-and-few-shot-llm-based-classification-for-hallucination-detection"},[e("span",null,"SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection")])],-1),Do=e("p",null,[e("strong",null,"Authors"),t(": Bradley P. Allen, Fina Polat, Paul Groth")],-1),Oo=e("strong",null,"Link",-1),Fo={href:"http://arxiv.org/abs/2404.03732v1",target:"_blank",rel:"noopener noreferrer"},jo=e("p",null,[e("strong",null,"Abstract"),t(": We describe the University of Amsterdam Intelligent Data Engineering Lab team's entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system builds on previous work on using prompt programming and in-context learning with large language models (LLMs) to build classifiers for hallucination detection, and extends that work through the incorporation of context-specific definition of task, role, and target concept, and automated generation of examples for use in a few-shot prompting approach. The resulting system achieved fourth-best and sixth-best performance in the model-agnostic track and model-aware tracks for Task 6, respectively, and evaluation using the validation sets showed that the system's classification decisions were consistent with those of the crowd-sourced human labellers. We further found that a zero-shot approach provided better accuracy than a few-shot approach using automatically generated examples. Code for the system described in this paper is available on Github.")],-1),Ko=e("h4",{id:"training-llms-over-neurally-compressed-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#training-llms-over-neurally-compressed-text"},[e("span",null,"Training LLMs over Neurally Compressed Text")])],-1),Yo=e("p",null,[e("strong",null,"Authors"),t(": Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant")],-1),No=e("strong",null,"Link",-1),Zo={href:"http://arxiv.org/abs/2404.03626v1",target:"_blank",rel:"noopener noreferrer"},Jo=e("p",null,[e("strong",null,"Abstract"),t(': In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.')],-1),Vo=e("h4",{id:"unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unveiling-llms-the-evolution-of-latent-representations-in-a-temporal-knowledge-graph"},[e("span",null,"Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph")])],-1),Uo=e("p",null,[e("strong",null,"Authors"),t(": Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini")],-1),Qo=e("strong",null,"Link",-1),Xo={href:"http://arxiv.org/abs/2404.03623v1",target:"_blank",rel:"noopener noreferrer"},$o=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of common factual knowledge information. However, unravelling the underlying reasoning of LLMs and explaining their internal mechanisms of exploiting this factual knowledge remain active areas of investigation. Our work analyzes the factual knowledge encoded in the latent representation of LLMs when prompted to assess the truthfulness of factual claims. We propose an end-to-end framework that jointly decodes the factual knowledge embedded in the latent space of LLMs from a vector space to a set of ground predicates and represents its evolution across the layers using a temporal knowledge graph. Our framework relies on the technique of activation patching which intervenes in the inference computation of a model by dynamically altering its latent representations. Consequently, we neither rely on external models nor training processes. We showcase our framework with local and global interpretability analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The local interpretability analysis exposes different latent errors from representation to multi-hop reasoning errors. On the other hand, the global analysis uncovered patterns in the underlying evolution of the model's factual knowledge (e.g., store-and-seek factual information). By enabling graph-based analyses of the latent representations, this work represents a step towards the mechanistic interpretability of LLMs.")],-1),es=e("h4",{id:"evaluating-llms-at-detecting-errors-in-llm-responses",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluating-llms-at-detecting-errors-in-llm-responses"},[e("span",null,"Evaluating LLMs at Detecting Errors in LLM Responses")])],-1),ts=e("p",null,[e("strong",null,"Authors"),t(": Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang")],-1),ns=e("strong",null,"Link",-1),as={href:"http://arxiv.org/abs/2404.03602v1",target:"_blank",rel:"noopener noreferrer"},is=e("p",null,[e("strong",null,"Abstract"),t(": With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.")],-1),os=e("h4",{id:"personalized-llm-response-generation-with-parameterized-memory-injection",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#personalized-llm-response-generation-with-parameterized-memory-injection"},[e("span",null,"Personalized LLM Response Generation with Parameterized Memory Injection")])],-1),ss=e("p",null,[e("strong",null,"Authors"),t(": Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu")],-1),rs=e("strong",null,"Link",-1),ls={href:"http://arxiv.org/abs/2404.03565v1",target:"_blank",rel:"noopener noreferrer"},cs=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, personalized LLM response generation holds the potential to offer substantial benefits for individuals in critical areas such as medical. Existing research has explored memory-augmented methods to prompt the LLM with pre-stored user-specific knowledge for personalized response generation in terms of new queries. We contend that such paradigm is unable to perceive fine-granularity information. In this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach using parameter-efficient fine-tuning (PEFT) and along with a Bayesian Optimisation searching strategy to achieve \\textbf{L}LM \\textbf{P}ersonalization(\\textbf{MiLP}).")],-1),hs=e("h4",{id:"minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#minigpt4-video-advancing-multimodal-llms-for-video-understanding-with-interleaved-visual-textual-tokens"},[e("span",null,"MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens")])],-1),ds=e("p",null,[e("strong",null,"Authors"),t(": Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny")],-1),us=e("strong",null,"Link",-1),gs={href:"http://arxiv.org/abs/2404.03413v1",target:"_blank",rel:"noopener noreferrer"},ps=e("p",null,[e("strong",null,"Abstract"),t(": This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM) designed specifically for video understanding. The model is capable of processing both temporal visual and textual data, making it adept at understanding the complexities of videos. Building upon the success of MiniGPT-v2, which excelled in translating visual features into the LLM space for single images and achieved impressive results on various image-text benchmarks, this paper extends the model's capabilities to process a sequence of frames, enabling it to comprehend videos. MiniGPT4-video does not only consider visual content but also incorporates textual conversations, allowing the model to effectively answer queries involving both visual and text components. The proposed model outperforms existing state-of-the-art methods, registering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks respectively. Our models and code have been made publicly available here https://vision-cair.github.io/MiniGPT4-video/")],-1),ms=e("h4",{id:"do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#do-large-language-models-rank-fairly-an-empirical-study-on-the-fairness-of-llms-as-rankers"},[e("span",null,"Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers")])],-1),fs=e("p",null,[e("strong",null,"Authors"),t(": Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang")],-1),vs=e("strong",null,"Link",-1),bs={href:"http://arxiv.org/abs/2404.03192v1",target:"_blank",rel:"noopener noreferrer"},ys=e("p",null,[e("strong",null,"Abstract"),t(": The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works (e.g., RankGPT) have also demonstrated that the LLMs exhibit better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.")],-1),Ls=e("h4",{id:"robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robust-pronoun-use-fidelity-with-english-llms-are-they-reasoning-repeating-or-just-biased"},[e("span",null,"Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?")])],-1),ws=e("p",null,[e("strong",null,"Authors"),t(": Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich Klakow")],-1),_s=e("strong",null,"Link",-1),ks={href:"http://arxiv.org/abs/2404.03134v1",target:"_blank",rel:"noopener noreferrer"},xs=e("p",null,[e("strong",null,"Abstract"),t(": Robust, faithful and harm-free pronoun use for individuals is an important goal for language models as their use increases, but prior work tends to study only one or two of these components at a time. To measure progress towards the combined goal, we introduce the task of pronoun use fidelity: given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later, independent of potential distractors. We present a carefully-designed dataset of over 5 million instances to evaluate pronoun use fidelity in English, and we use it to evaluate 37 popular large language models across architectures (encoder-only, decoder-only and encoder-decoder) and scales (11M-70B parameters). We find that while models can mostly faithfully reuse previously-specified pronouns in the presence of no distractors, they are significantly worse at processing she/her/her, singular they and neopronouns. Additionally, models are not robustly faithful to pronouns, as they are easily distracted. With even one additional sentence containing a distractor pronoun, accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops by 52% for decoder-only models and 13% for encoder-only models. We show that widely-used large language models are still brittle, with large gaps in reasoning and in processing different pronouns in a setting that is very simple for humans, and we encourage researchers in bias and reasoning to bridge them.")],-1),Ms=e("h4",{id:"towards-standards-compliant-assistive-technology-product-specifications-via-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-standards-compliant-assistive-technology-product-specifications-via-llms"},[e("span",null,"Towards Standards-Compliant Assistive Technology Product Specifications via LLMs")])],-1),As=e("p",null,[e("strong",null,"Authors"),t(": Chetan Arora, John Grundy, Louise Puli, Natasha Layton")],-1),Ts=e("strong",null,"Link",-1),Cs={href:"http://arxiv.org/abs/2404.03122v1",target:"_blank",rel:"noopener noreferrer"},Ss=e("p",null,[e("strong",null,"Abstract"),t(": In the rapidly evolving field of assistive technology (AT), ensuring that products meet national and international standards is essential for user safety, efficacy, and accessibility. In this vision paper, we introduce CompliAT, a pioneering framework designed to streamline the compliance process of AT product specifications with these standards through the innovative use of Large Language Models (LLMs). CompliAT addresses three critical tasks: checking terminology consistency, classifying products according to standards, and tracing key product specifications to standard requirements. We tackle the challenge of terminology consistency to ensure that the language used in product specifications aligns with relevant standards, reducing misunderstandings and non-compliance risks. We propose a novel approach for product classification, leveraging a retrieval-augmented generation model to accurately categorize AT products aligning to international standards, despite the sparse availability of training data. Finally, CompliAT implements a traceability and compliance mechanism from key product specifications to standard requirements, ensuring all aspects of an AT product are thoroughly vetted against the corresponding standards. By semi-automating these processes, CompliAT aims to significantly reduce the time and effort required for AT product standards compliance and uphold quality and safety standards. We outline our planned implementation and evaluation plan for CompliAT.")],-1),Is=e("h2",{id:"_2024-04-03",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-03"},[e("span",null,"2024-04-03")])],-1),zs=e("h4",{id:"the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies"},[e("span",null,"The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies")])],-1),Es=e("p",null,[e("strong",null,"Authors"),t(": Marcin P. Joachimiak, Mark A. Miller, J. Harry Caufield, Ryan Ly, Nomi L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard")],-1),qs=e("strong",null,"Link",-1),Ps={href:"http://arxiv.org/abs/2404.03044v1",target:"_blank",rel:"noopener noreferrer"},Ws=e("p",null,[e("strong",null,"Abstract"),t(": The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of large language models (LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. The ontology is structured around six top-level branches: Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI. AIO's development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support. This approach not only ensures the ontology's relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies. The ontology's utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal (https://bioportal.bioontology.org/ontologies/AIO).")],-1),Gs=e("h4",{id:"i-design-personalized-llm-interior-designer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-design-personalized-llm-interior-designer"},[e("span",null,"I-Design: Personalized LLM Interior Designer")])],-1),Rs=e("p",null,[e("strong",null,"Authors"),t(": Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang")],-1),Hs=e("strong",null,"Link",-1),Bs={href:"http://arxiv.org/abs/2404.02838v1",target:"_blank",rel:"noopener noreferrer"},Ds=e("p",null,[e("strong",null,"Abstract"),t(": Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of large language model agents that engage in dialogues and logical reasoning with one another, transforming textual user input into feasible scene graph designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a vision-language model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.")],-1),Os=e("h4",{id:"aqua-combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#aqua-combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms"},[e("span",null,"AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs")])],-1),Fs=e("p",null,[e("strong",null,"Authors"),t(": Maike Behrendt, Stefan Sylvius Wagner, Marc Ziegele, Lena Wilms, Anke Stoll, Dominique Heinbach, Stefan Harmeling")],-1),js=e("strong",null,"Link",-1),Ks={href:"http://arxiv.org/abs/2404.02761v1",target:"_blank",rel:"noopener noreferrer"},Ys=e("p",null,[e("strong",null,"Abstract"),t(": Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts' vs. non-experts' annotations confirms theoretical findings in the social science literature.")],-1),Ns=e("h4",{id:"unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm"},[e("span",null,"Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM")])],-1),Zs=e("p",null,[e("strong",null,"Authors"),t(": Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yuekai Huang, Jun Hu, Qing Wang")],-1),Js=e("strong",null,"Link",-1),Vs={href:"http://arxiv.org/abs/2404.02706v1",target:"_blank",rel:"noopener noreferrer"},Us=e("p",null,[e("strong",null,"Abstract"),t(": Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers' lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an LLM-based hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses in-context learning to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high BLEU and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: https://youtu.be/FWgfcctRbfI.")],-1),Qs=e("h4",{id:"improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation"},[e("span",null,"Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation")])],-1),Xs=e("p",null,[e("strong",null,"Authors"),t(": Yizhu Liu, Ran Tao, Shengyu Guo, Yifan Yang")],-1),$s=e("strong",null,"Link",-1),er={href:"http://arxiv.org/abs/2404.02616v1",target:"_blank",rel:"noopener noreferrer"},tr=e("p",null,[e("strong",null,"Abstract"),t(": Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user's requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training data for search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query from queries and documents in existing training data, which can construct new query-document pairs as training data. Extensive offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling.")],-1),nr=e("h4",{id:"learn-to-disguise-avoid-refusal-responses-in-llm-s-defense-via-a-multi-agent-attacker-disguiser-game",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learn-to-disguise-avoid-refusal-responses-in-llm-s-defense-via-a-multi-agent-attacker-disguiser-game"},[e("span",null,"Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game")])],-1),ar=e("p",null,[e("strong",null,"Authors"),t(": Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li")],-1),ir=e("strong",null,"Link",-1),or={href:"http://arxiv.org/abs/2404.02532v1",target:"_blank",rel:"noopener noreferrer"},sr=e("p",null,[e("strong",null,"Abstract"),t(": With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model's ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.")],-1),rr=e("h4",{id:"utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers"},[e("span",null,"uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?")])],-1),lr=e("p",null,[e("strong",null,"Authors"),t(": Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh")],-1),cr=e("strong",null,"Link",-1),hr={href:"http://arxiv.org/abs/2404.02474v1",target:"_blank",rel:"noopener noreferrer"},dr=e("p",null,[e("strong",null,"Abstract"),t(": Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. Furthermore, fine-tuning Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.")],-1),ur=e("h4",{id:"enhancing-low-resource-llms-classification-with-peft-and-synthetic-data",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-low-resource-llms-classification-with-peft-and-synthetic-data"},[e("span",null,"Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data")])],-1),gr=e("p",null,[e("strong",null,"Authors"),t(": Parth Patwa, Simone Filice, Zhiyu Chen, Giuseppe Castellucci, Oleg Rokhlenko, Shervin Malmasi")],-1),pr=e("strong",null,"Link",-1),mr={href:"http://arxiv.org/abs/2404.02422v1",target:"_blank",rel:"noopener noreferrer"},fr=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.")],-1),vr=e("h2",{id:"_2024-04-02",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-02"},[e("span",null,"2024-04-02")])],-1),br=e("h4",{id:"constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#constrained-robotic-navigation-on-preferred-terrains-using-llms-and-speech-instruction-exploiting-the-power-of-adverbs"},[e("span",null,"Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs")])],-1),yr=e("p",null,[e("strong",null,"Authors"),t(": Faraz Lotfi, Farnoosh Faraji, Nikhil Kakodkar, Travis Manderson, David Meger, Gregory Dudek")],-1),Lr=e("strong",null,"Link",-1),wr={href:"http://arxiv.org/abs/2404.02294v1",target:"_blank",rel:"noopener noreferrer"},_r=e("p",null,[e("strong",null,"Abstract"),t(": This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.")],-1),kr=e("h4",{id:"llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-in-the-loop-leveraging-large-language-model-annotations-for-active-learning-in-low-resource-languages"},[e("span",null,"LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages")])],-1),xr=e("p",null,[e("strong",null,"Authors"),t(": Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer")],-1),Mr=e("strong",null,"Link",-1),Ar={href:"http://arxiv.org/abs/2404.02261v1",target:"_blank",rel:"noopener noreferrer"},Tr=e("p",null,[e("strong",null,"Abstract"),t(": Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential cost savings of at least 42.45 times compared to human annotation. Our proposed solution shows promising potential to substantially reduce both the monetary and computational costs associated with automation in low-resource settings. By bridging the gap between low-resource languages and AI, this approach fosters broader inclusion and shows the potential to enable automation across diverse linguistic landscapes.")],-1),Cr=e("h4",{id:"jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks"},[e("span",null,"Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks")])],-1),Sr=e("p",null,[e("strong",null,"Authors"),t(": Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion")],-1),Ir=e("strong",null,"Link",-1),zr={href:"http://arxiv.org/abs/2404.02151v1",target:"_blank",rel:"noopener noreferrer"},Er=e("p",null,[e("strong",null,"Abstract"),t(`: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.`)],-1),qr=e("h4",{id:"topic-based-watermarks-for-llm-generated-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#topic-based-watermarks-for-llm-generated-text"},[e("span",null,"Topic-based Watermarks for LLM-Generated Text")])],-1),Pr=e("p",null,[e("strong",null,"Authors"),t(": Alexander Nemecek, Yuzhou Jiang, Erman Ayday")],-1),Wr=e("strong",null,"Link",-1),Gr={href:"http://arxiv.org/abs/2404.02138v1",target:"_blank",rel:"noopener noreferrer"},Rr=e("p",null,[e("strong",null,"Abstract"),t(': Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked LLM. Inspired from previous work, we propose using a pair of lists (that are generated based on the specified extracted topic(s)) that specify certain tokens to be included or excluded while generating the watermarked output of the LLM. Using the proposed watermarking algorithm, we show the practicality of a watermark detection algorithm. Furthermore, we discuss a wide range of attacks that can emerge against watermarking algorithms for LLMs and the benefit of the proposed watermarking scheme for the feasibility of modeling a potential attacker considering its benefit vs. loss.')],-1),Hr=e("h4",{id:"advancing-llm-reasoning-generalists-with-preference-trees",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#advancing-llm-reasoning-generalists-with-preference-trees"},[e("span",null,"Advancing LLM Reasoning Generalists with Preference Trees")])],-1),Br=e("p",null,[e("strong",null,"Authors"),t(": Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun")],-1),Dr=e("strong",null,"Link",-1),Or={href:"http://arxiv.org/abs/2404.02078v1",target:"_blank",rel:"noopener noreferrer"},Fr=e("p",null,[e("strong",null,"Abstract"),t(": We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. UltraInteract allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with UltraInteract, leads to a strong reward model.")],-1),jr=e("h4",{id:"long-context-llms-struggle-with-long-in-context-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#long-context-llms-struggle-with-long-in-context-learning"},[e("span",null,"Long-context LLMs Struggle with Long In-context Learning")])],-1),Kr=e("p",null,[e("strong",null,"Authors"),t(": Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen")],-1),Yr=e("strong",null,"Link",-1),Nr={href:"http://arxiv.org/abs/2404.02060v1",target:"_blank",rel:"noopener noreferrer"},Zr=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However, after the context window exceeds 20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap in current LLM capabilities for processing and understanding long, context-rich sequences. Further analysis revealed a tendency among models to favor predictions for labels presented towards the end at the sequence. Their ability to reason over multiple pieces in the long sequence is yet to be improved. Our study reveals that long context understanding and reasoning is still a challenging task for the existing LLMs. We believe LIConBench could serve as a more realistic evaluation for the future long context LLMs.")],-1),Jr=e("h4",{id:"multitask-based-evaluation-of-open-source-llm-on-software-vulnerability",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#multitask-based-evaluation-of-open-source-llm-on-software-vulnerability"},[e("span",null,"Multitask-based Evaluation of Open-Source LLM on Software Vulnerability")])],-1),Vr=e("p",null,[e("strong",null,"Authors"),t(": Xin Yin, Chao Ni")],-1),Ur=e("strong",null,"Link",-1),Qr={href:"http://arxiv.org/abs/2404.02056v1",target:"_blank",rel:"noopener noreferrer"},Xr=e("p",null,[e("strong",null,"Abstract"),t(": This paper proposes a pipeline for quantitatively evaluating interactive LLMs using publicly available datasets. We carry out an extensive technical evaluation of LLMs using Big-Vul covering four different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of LLMs based on this dataset. We find that the existing state-of-the-art methods are generally superior to LLMs in software vulnerability detection. Although LLMs improve accuracy when providing context information, they still have limitations in accurately predicting severity ratings for certain CWE types. In addition, LLMs demonstrate some ability to locate vulnerabilities for certain CWE types, but their performance varies among different CWE types. Finally, LLMs show uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in a few-shot setting. Overall, though LLMs perform well in some aspects, they still need improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities to fully realize their potential. Our evaluation pipeline provides valuable insights for further enhancing LLMs' software vulnerability handling capabilities.")],-1),$r=e("h4",{id:"muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#muxserve-flexible-multiplexing-for-efficient-multiple-llm-serving"},[e("span",null,"MuxServe: Flexible Multiplexing for Efficient Multiple LLM Serving")])],-1),el=e("p",null,[e("strong",null,"Authors"),t(": Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion Stoica, Hao Zhang")],-1),tl=e("strong",null,"Link",-1),nl={href:"http://arxiv.org/abs/2404.02015v1",target:"_blank",rel:"noopener noreferrer"},al=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have demon- strated remarkable performance, and organiza- tions are racing to serve LLMs of varying sizes as endpoints for use-cases like chat, programming and search. However, efficiently serving multiple LLMs poses significant challenges for existing approaches due to varying popularity of LLMs. In the paper, we present MuxServe, a flexible spatial-temporal multiplexing system for efficient multiple LLM serving. The key insight behind is to colocate LLMs considering their popularity to multiplex memory resources, and leverage the characteristics of prefill and decoding phases to separate and flexibly colocate them to multiplex computation resources. MuxServe formally for- mulates the multiplexing problem, and proposes a novel placement algorithm and adaptive batch scheduling strategy to identify optimal coloca- tions and maximize utilization. MuxServe de- signs a unified resource manager to enable flexi- ble and efficient multiplexing. Evaluation results show that MuxServe can achieves up to $1.8\\times$ higher throughput or processes $2.9\\times$ more requests within $99%$ SLO attainment.")],-1),il=e("h4",{id:"self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-organized-agents-a-llm-multi-agent-framework-toward-ultra-large-scale-code-generation-and-optimization"},[e("span",null,"Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization")])],-1),ol=e("p",null,[e("strong",null,"Authors"),t(": Yoichi Ishibashi, Yoshimasa Nishimura")],-1),sl=e("strong",null,"Link",-1),rl={href:"http://arxiv.org/abs/2404.02183v1",target:"_blank",rel:"noopener noreferrer"},ll=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.")],-1),cl=e("h4",{id:"towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-better-understanding-of-cybercrime-the-role-of-fine-tuned-llms-in-translation"},[e("span",null,"Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs in Translation")])],-1),hl=e("p",null,[e("strong",null,"Authors"),t(": Veronica Valeros, Anna Širokova, Carlos Catania, Sebastian Garcia")],-1),dl=e("strong",null,"Link",-1),ul={href:"http://arxiv.org/abs/2404.01940v1",target:"_blank",rel:"noopener noreferrer"},gl=e("p",null,[e("strong",null,"Abstract"),t(": Understanding cybercrime communications is paramount for cybersecurity defence. This often involves translating communications into English for processing, interpreting, and generating timely intelligence. The problem is that translation is hard. Human translation is slow, expensive, and scarce. Machine translation is inaccurate and biased. We propose using fine-tuned Large Language Models (LLM) to generate translations that can accurately capture the nuances of cybercrime language. We apply our technique to public chats from the NoName057(16) Russian-speaking hacktivist group. Our results show that our fine-tuned LLM model is better, faster, more accurate, and able to capture nuances of the language. Our method shows it is possible to achieve high-fidelity translations and significantly reduce costs by a factor ranging from 430 to 23,000 compared to a human translator.")],-1),pl=e("h4",{id:"where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#where-to-move-next-zero-shot-generalization-of-llms-for-next-poi-recommendation"},[e("span",null,"Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation")])],-1),ml=e("p",null,[e("strong",null,"Authors"),t(": Shanshan Feng, Haoming Lyu, Caishun Chen, Yew-Soon Ong")],-1),fl=e("strong",null,"Link",-1),vl={href:"http://arxiv.org/abs/2404.01855v1",target:"_blank",rel:"noopener noreferrer"},bl=e("p",null,[e("strong",null,"Abstract"),t(": Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.")],-1),yl=e("h4",{id:"great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack"},[e("span",null,"Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack")])],-1),Ll=e("p",null,[e("strong",null,"Authors"),t(": Mark Russinovich, Ahmed Salem, Ronen Eldan")],-1),wl=e("strong",null,"Link",-1),_l={href:"http://arxiv.org/abs/2404.01833v1",target:"_blank",rel:"noopener noreferrer"},kl=e("p",null,[e("strong",null,"Abstract"),t(`: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we introduce Crescendomation, a tool that automates the Crescendo attack, and our evaluation showcases its effectiveness against state-of-the-art models.`)],-1),xl=e("h4",{id:"transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#transforming-llms-into-cross-modal-and-cross-lingual-retrieval-systems"},[e("span",null,"Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems")])],-1),Ml=e("p",null,[e("strong",null,"Authors"),t(": Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego")],-1),Al=e("strong",null,"Link",-1),Tl={href:"http://arxiv.org/abs/2404.01616v2",target:"_blank",rel:"noopener noreferrer"},Cl=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.")],-1),Sl=e("h4",{id:"transforming-llms-into-cross-modal-and-cross-lingual-retrievalsystems",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#transforming-llms-into-cross-modal-and-cross-lingual-retrievalsystems"},[e("span",null,"Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems")])],-1),Il=e("p",null,[e("strong",null,"Authors"),t(": Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer, Siddharth Dalmia, Gustavo Hernandez Abrego")],-1),zl=e("strong",null,"Link",-1),El={href:"http://arxiv.org/abs/2404.01616v1",target:"_blank",rel:"noopener noreferrer"},ql=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.")],-1),Pl=e("h2",{id:"_2024-04-01",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-04-01"},[e("span",null,"2024-04-01")])],-1),Wl=e("h4",{id:"syntactic-robustness-for-llm-based-code-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#syntactic-robustness-for-llm-based-code-generation"},[e("span",null,"Syntactic Robustness for LLM-based Code Generation")])],-1),Gl=e("p",null,[e("strong",null,"Authors"),t(": Laboni Sarker, Mara Downing, Achintya Desai, Tevfik Bultan")],-1),Rl=e("strong",null,"Link",-1),Hl={href:"http://arxiv.org/abs/2404.01535v1",target:"_blank",rel:"noopener noreferrer"},Bl=e("p",null,[e("strong",null,"Abstract"),t(": Rapid advances in the field of Large Language Models (LLMs) have made LLM-based code generation an important area for investigation. An LLM-based code generator takes a prompt as input and produces code that implements the requirements specified in the prompt. Many software requirements include mathematical formulas that specify the expected behavior of the code to be generated. Given a code generation prompt that includes a mathematical formula, a reasonable expectation is that, if the formula is syntactically modified without changing its semantics, the generated code for the modified prompt should be semantically equivalent. We formalize this concept as syntactic robustness and investigate the syntactic robustness of GPT-3.5-Turbo and GPT-4 as code generators. To test syntactic robustness, we generate syntactically different but semantically equivalent versions of prompts using a set of mutators that only modify mathematical formulas in prompts. In this paper, we focus on prompts that ask for code that generates solutions to variables in an equation, when given coefficients of the equation as input. Our experimental evaluation demonstrates that GPT-3.5-Turbo and GPT-4 are not syntactically robust for this type of prompts. To improve syntactic robustness, we define a set of reductions that transform the formulas to a simplified form and use these reductions as a pre-processing step. Our experimental results indicate that the syntactic robustness of LLM-based code generation can be improved using our approach.")],-1),Dl=e("h4",{id:"will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#will-the-real-linda-please-stand-up-to-large-language-models-examining-the-representativeness-heuristic-in-llms"},[e("span",null,"Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs")])],-1),Ol=e("p",null,[e("strong",null,"Authors"),t(": Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald")],-1),Fl=e("strong",null,"Link",-1),jl={href:"http://arxiv.org/abs/2404.01461v1",target:"_blank",rel:"noopener noreferrer"},Kl=e("p",null,[e("strong",null,"Abstract"),t(": Although large language models (LLMs) have demonstrated remarkable proficiency in understanding text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example versus considering broader facts or statistical evidence. This work investigates the impact of the representativeness heuristic on LLM reasoning. We created REHEAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to REHEAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model of using its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while failing in a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.")],-1),Yl=e("h4",{id:"unveiling-divergent-inductive-biases-of-llms-on-temporal-data",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#unveiling-divergent-inductive-biases-of-llms-on-temporal-data"},[e("span",null,"Unveiling Divergent Inductive Biases of LLMs on Temporal Data")])],-1),Nl=e("p",null,[e("strong",null,"Authors"),t(": Sindhu Kishore, Hangfeng He")],-1),Zl=e("strong",null,"Link",-1),Jl={href:"http://arxiv.org/abs/2404.01453v1",target:"_blank",rel:"noopener noreferrer"},Vl=e("p",null,[e("strong",null,"Abstract"),t(`: Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA format for both implicit and explicit events, while GPT-4 leans towards "BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards "TRUE'', and GPT-4 exhibits a preference for "FALSE'' in the TE format for both implicit and explicit events. This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.`)],-1),Ul=e("h4",{id:"position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#position-aware-parameter-efficient-fine-tuning-approach-for-reducing-positional-bias-in-llms"},[e("span",null,"Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs")])],-1),Ql=e("p",null,[e("strong",null,"Authors"),t(": Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao, Chengyuan Ma, Liang Zhao, Yang Liu")],-1),Xl=e("strong",null,"Link",-1),$l={href:"http://arxiv.org/abs/2404.01430v1",target:"_blank",rel:"noopener noreferrer"},ec=e("p",null,[e("strong",null,"Abstract"),t(": Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augmentation technique and a parameter efficient adapter, enhancing a uniform attention distribution across the input context. Our experiments demonstrate that the proposed approach effectively reduces positional bias, improving LLMs' effectiveness in handling long context sequences for various tasks that require externally retrieved knowledge.")],-1),tc=e("h4",{id:"a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-preliminary-roadmap-for-llms-as-assistants-in-exploring-analyzing-and-visualizing-knowledge-graphs"},[e("span",null,"A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs")])],-1),nc=e("p",null,[e("strong",null,"Authors"),t(": Harry Li, Gabriel Appleby, Ashley Suh")],-1),ac=e("strong",null,"Link",-1),ic={href:"http://arxiv.org/abs/2404.01425v1",target:"_blank",rel:"noopener noreferrer"},oc=e("p",null,[e("strong",null,"Abstract"),t(": We present a mixed-methods study to explore how large language models (LLMs) can assist users in the visual exploration and analysis of knowledge graphs (KGs). We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with KGs and LLMs, either collaboratively or concurrently. Our findings show that participants overwhelmingly want an LLM to facilitate data retrieval from KGs through joint query construction, to identify interesting relationships in the KG through multi-turn conversation, and to create on-demand visualizations from the KG that enhance their trust in the LLM's outputs. To interact with an LLM, participants strongly prefer a chat-based 'widget,' built on top of their regular analysis workflows, with the ability to guide the LLM using their interactions with a visualization. When viewing an LLM's outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the KG) alongside summarizing text. However, participants also expressed concerns about an LLM's ability to maintain semantic intent when translating natural language questions into KG queries, the risk of an LLM 'hallucinating' false data from the KG, and the difficulties of engineering a 'perfect prompt.' From the analysis of our interviews, we contribute a preliminary roadmap for the design of LLM-driven knowledge graph exploration systems and outline future opportunities in this emergent design space.")],-1),sc=e("h4",{id:"prompt-prompted-mixture-of-experts-for-efficient-llm-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#prompt-prompted-mixture-of-experts-for-efficient-llm-generation"},[e("span",null,"Prompt-prompted Mixture of Experts for Efficient LLM Generation")])],-1),rc=e("p",null,[e("strong",null,"Authors"),t(": Harry Dong, Beidi Chen, Yuejie Chi")],-1),lc=e("strong",null,"Link",-1),cc={href:"http://arxiv.org/abs/2404.01365v1",target:"_blank",rel:"noopener noreferrer"},hc=e("p",null,[e("strong",null,"Abstract"),t(": With the development of transformer-based large language models (LLMs), they have been applied to many fields due to their remarkable utility, but this comes at a considerable computational cost at deployment. Fortunately, some methods such as pruning or constructing a mixture of experts (MoE) aim at exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in speed and reduction in memory requirements. However, these techniques can be very costly and inflexible in practice, as they often require training or are restricted to specific types of architectures. To address this, we introduce GRIFFIN, a novel training-free MoE that selects unique FF experts at the sequence level for efficient generation across a plethora of LLMs with different non-ReLU activation functions. This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking. Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.25$\\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code will be available at https://github.com/hdong920/GRIFFIN.")],-1),dc=e("h4",{id:"mapping-the-increasing-use-of-llms-in-scientific-papers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mapping-the-increasing-use-of-llms-in-scientific-papers"},[e("span",null,"Mapping the Increasing Use of LLMs in Scientific Papers")])],-1),uc=e("p",null,[e("strong",null,"Authors"),t(": Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang, Christopher Potts, Christopher D Manning, James Y. Zou")],-1),gc=e("strong",null,"Link",-1),pc={href:"http://arxiv.org/abs/2404.01268v1",target:"_blank",rel:"noopener noreferrer"},mc=e("p",null,[e("strong",null,"Abstract"),t(": Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and is more robust than inference on individual instances. Our findings reveal a steady increase in LLM usage, with the largest and fastest growth observed in Computer Science papers (up to 17.5%). In comparison, Mathematics papers and the Nature portfolio showed the least LLM modification (up to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher levels of LLM-modification are associated with papers whose first authors post preprints more frequently, papers in more crowded research areas, and papers of shorter lengths. Our findings suggests that LLMs are being broadly used in scientific writings.")],-1),fc=e("h4",{id:"llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-as-a-mastermind-a-survey-of-strategic-reasoning-with-large-language-models"},[e("span",null,"LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models")])],-1),vc=e("p",null,[e("strong",null,"Authors"),t(": Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei")],-1),bc=e("strong",null,"Link",-1),yc={href:"http://arxiv.org/abs/2404.01230v1",target:"_blank",rel:"noopener noreferrer"},Lc=e("p",null,[e("strong",null,"Abstract"),t(": This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.")],-1),wc=e("h4",{id:"detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#detect2interact-localizing-object-key-field-in-visual-question-answering-vqa-with-llms"},[e("span",null,"Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs")])],-1),_c=e("p",null,[e("strong",null,"Authors"),t(": Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo")],-1),kc=e("strong",null,"Link",-1),xc={href:"http://arxiv.org/abs/2404.01151v1",target:"_blank",rel:"noopener noreferrer"},Mc=e("p",null,[e("strong",null,"Abstract"),t(`: Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce "Detect2Interact", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.`)],-1),Ac=e("h4",{id:"do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#do-llms-find-human-answers-to-fact-driven-questions-perplexing-a-case-study-on-reddit"},[e("span",null,"Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit")])],-1),Tc=e("p",null,[e("strong",null,"Authors"),t(": Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak, Sarah Masud Preum")],-1),Cc=e("strong",null,"Link",-1),Sc={href:"http://arxiv.org/abs/2404.01147v1",target:"_blank",rel:"noopener noreferrer"},Ic=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.")],-1),zc=e("h4",{id:"structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#structured-information-matters-incorporating-abstract-meaning-representation-into-llms-for-improved-open-domain-dialogue-evaluation"},[e("span",null,"Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation")])],-1),Ec=e("p",null,[e("strong",null,"Authors"),t(": Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin")],-1),qc=e("strong",null,"Link",-1),Pc={href:"http://arxiv.org/abs/2404.01129v1",target:"_blank",rel:"noopener noreferrer"},Wc=e("p",null,[e("strong",null,"Abstract"),t(": Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs) with LLMs. The SLMs can explicitly incorporate Abstract Meaning Representation (AMR) graph information of the dialogue through a gating mechanism for enhanced semantic representation learning. The evaluation result of SLMs and AMR graph information are plugged into the prompt of LLM, for the enhanced in-context learning performance. Experimental results on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at https://github.com/Bernard-Yang/SIMAMR.")],-1),Gc=e("h4",{id:"llm-attributor-interactive-visual-attribution-for-llm-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-attributor-interactive-visual-attribution-for-llm-generation"},[e("span",null,"LLM Attributor: Interactive Visual Attribution for LLM Generation")])],-1),Rc=e("p",null,[e("strong",null,"Authors"),t(": Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng")],-1),Hc=e("strong",null,"Link",-1),Bc={href:"http://arxiv.org/abs/2404.01361v1",target:"_blank",rel:"noopener noreferrer"},Dc=e("p",null,[e("strong",null,"Abstract"),t(": While large language models (LLMs) have shown remarkable capability to generate convincing text across diverse domains, concerns around its potential risks have highlighted the importance of understanding the rationale behind text generation. We present LLM Attributor, a Python library that provides interactive visualizations for training data attribution of an LLM's text generation. Our library offers a new way to quickly attribute an LLM's text generation to training data points to inspect model behaviors, enhance its trustworthiness, and compare model-generated text with user-provided text. We describe the visual and interactive design of our tool and highlight usage scenarios for LLaMA2 models fine-tuned with two different datasets: online articles about recent disasters and finance-related question-answer pairs. Thanks to LLM Attributor's broad support for computational notebooks, users can easily integrate it into their workflow to interactively visualize attributions of their models. For easier access and extensibility, we open-source LLM Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is available at https://youtu.be/mIG2MDQKQxM.")],-1),Oc=e("h4",{id:"enabling-memory-safety-of-c-programs-using-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enabling-memory-safety-of-c-programs-using-llms"},[e("span",null,"Enabling Memory Safety of C Programs using LLMs")])],-1),Fc=e("p",null,[e("strong",null,"Authors"),t(": Nausheen Mohammed, Akash Lal, Aseem Rastogi, Subhajit Roy, Rahul Sharma")],-1),jc=e("strong",null,"Link",-1),Kc={href:"http://arxiv.org/abs/2404.01096v1",target:"_blank",rel:"noopener noreferrer"},Yc=e("p",null,[e("strong",null,"Abstract"),t(": Memory safety violations in low-level code, written in languages like C, continues to remain one of the major sources of software vulnerabilities. One method of removing such violations by construction is to port C code to a safe C dialect. Such dialects rely on programmer-supplied annotations to guarantee safety with minimal runtime overhead. This porting, however, is a manual process that imposes significant burden on the programmer and, hence, there has been limited adoption of this technique. The task of porting not only requires inferring annotations, but may also need refactoring/rewriting of the code to make it amenable to such annotations. In this paper, we use Large Language Models (LLMs) towards addressing both these concerns. We show how to harness LLM capabilities to do complex code reasoning as well as rewriting of large codebases. We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM. We implement our ideas in a tool called MSA that targets the CheckedC dialect. We evaluate MSA on several micro-benchmarks, as well as real-world code ranging up to 20K lines of code. We showcase superior performance compared to a vanilla LLM baseline, as well as demonstrate improvement over a state-of-the-art symbolic (non-LLM) technique.")],-1),Nc=e("h4",{id:"can-llms-get-help-from-other-llms-without-revealing-private-information",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-get-help-from-other-llms-without-revealing-private-information"},[e("span",null,"Can LLMs get help from other LLMs without revealing private information?")])],-1),Zc=e("p",null,[e("strong",null,"Authors"),t(": Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor Cărbune, Blaise Aguera y Arcas")],-1),Jc=e("strong",null,"Link",-1),Vc={href:"http://arxiv.org/abs/2404.01041v2",target:"_blank",rel:"noopener noreferrer"},Uc=e("p",null,[e("strong",null,"Abstract"),t(": Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language. Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.")],-1),Qc=e("h4",{id:"efficiently-distilling-llms-for-edge-applications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#efficiently-distilling-llms-for-edge-applications"},[e("span",null,"Efficiently Distilling LLMs for Edge Applications")])],-1),Xc=e("p",null,[e("strong",null,"Authors"),t(": Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih Lee")],-1),$c=e("strong",null,"Link",-1),eh={href:"http://arxiv.org/abs/2404.01353v1",target:"_blank",rel:"noopener noreferrer"},th=e("p",null,[e("strong",null,"Abstract"),t(": Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.")],-1),nh=e("h4",{id:"exploring-and-evaluating-hallucinations-in-llm-powered-code-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-and-evaluating-hallucinations-in-llm-powered-code-generation"},[e("span",null,"Exploring and Evaluating Hallucinations in LLM-Powered Code Generation")])],-1),ah=e("p",null,[e("strong",null,"Authors"),t(": Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang")],-1),ih=e("strong",null,"Link",-1),oh={href:"http://arxiv.org/abs/2404.00971v1",target:"_blank",rel:"noopener noreferrer"},sh=e("p",null,[e("strong",null,"Abstract"),t(": The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinations depending on the conflicting objectives and varying degrees of deviation observed in code generation. Furthermore, we systematically analyzed the distribution of hallucinations, exploring variations among different LLMs and their correlation with code correctness. Based on the results, we proposed HalluCode, a benchmark for evaluating the performance of code LLMs in recognizing hallucinations. Hallucination recognition and mitigation experiments with HalluCode and HumanEval show existing LLMs face great challenges in recognizing hallucinations, particularly in identifying their types, and are hardly able to mitigate hallucinations. We believe our findings will shed light on future research about hallucination evaluation, detection, and mitigation, ultimately paving the way for building more effective and reliable code LLMs in the future.")],-1),rh=e("h4",{id:"llms-are-good-sign-language-translators",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-are-good-sign-language-translators"},[e("span",null,"LLMs are Good Sign Language Translators")])],-1),lh=e("p",null,[e("strong",null,"Authors"),t(": Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, Jun Liu")],-1),ch=e("strong",null,"Link",-1),hh={href:"http://arxiv.org/abs/2404.00925v1",target:"_blank",rel:"noopener noreferrer"},dh=e("p",null,[e("strong",null,"Abstract"),t(": Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.")],-1),uh=e("h4",{id:"tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tm-trek-at-semeval-2024-task-8-towards-llm-based-automatic-boundary-detection-for-human-machine-mixed-text"},[e("span",null,"TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary Detection for Human-Machine Mixed Text")])],-1),gh=e("p",null,[e("strong",null,"Authors"),t(": Xiaoyan Qu, Xiangfeng Meng")],-1),ph=e("strong",null,"Link",-1),mh={href:"http://arxiv.org/abs/2404.00899v1",target:"_blank",rel:"noopener noreferrer"},fh=e("p",null,[e("strong",null,"Abstract"),t(": With the increasing prevalence of text generated by large language models (LLMs), there is a growing concern about distinguishing between LLM-generated and human-written texts in order to prevent the misuse of LLMs, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content. This paper explores LLMs' ability to identify boundaries in human-written and machine-generated mixed texts. We approach this task by transforming it into a token classification problem and regard the label turning point as the boundary. Notably, our ensemble model of LLMs achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of the SemEval'24 Competition Task 8. Additionally, we investigate factors that influence the capability of LLMs in detecting boundaries within mixed texts, including the incorporation of extra layers on top of LLMs, combination of segmentation loss, and the impact of pretraining. Our findings aim to provide valuable insights for future research in this area.")],-1),vh=e("h2",{id:"_2024-03-31",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-31"},[e("span",null,"2024-03-31")])],-1),bh=e("h4",{id:"the-larger-the-better-improved-llm-code-generation-via-budget-reallocation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-larger-the-better-improved-llm-code-generation-via-budget-reallocation"},[e("span",null,"The Larger the Better? Improved LLM Code-Generation via Budget Reallocation")])],-1),yh=e("p",null,[e("strong",null,"Authors"),t(": Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi")],-1),Lh=e("strong",null,"Link",-1),wh={href:"http://arxiv.org/abs/2404.00725v1",target:"_blank",rel:"noopener noreferrer"},_h=e("p",null,[e("strong",null,"Abstract"),t(": It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.")],-1),kh=e("h4",{id:"training-free-semantic-segmentation-via-llm-supervision",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#training-free-semantic-segmentation-via-llm-supervision"},[e("span",null,"Training-Free Semantic Segmentation via LLM-Supervision")])],-1),xh=e("p",null,[e("strong",null,"Authors"),t(": Wenfang Sun, Yingjun Du, Gaowen Liu, Ramana Kompella, Cees G. M. Snoek")],-1),Mh=e("strong",null,"Link",-1),Ah={href:"http://arxiv.org/abs/2404.00701v1",target:"_blank",rel:"noopener noreferrer"},Th=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in open vocabulary models, like CLIP, have notably advanced zero-shot classification and segmentation by utilizing natural language for class-specific embeddings. However, most research has focused on improving model accuracy through prompt engineering, prompt learning, or fine-tuning with limited labeled data, thereby overlooking the importance of refining the class descriptors. This paper introduces a new approach to text-supervised semantic segmentation using supervision by a large language model (LLM) that does not require extra training. Our method starts from an LLM, like GPT-3, to generate a detailed set of subclasses for more accurate class representation. We then employ an advanced text-supervised semantic segmentation model to apply the generated subclasses as target labels, resulting in diverse segmentation results tailored to each subclass's unique characteristics. Additionally, we propose an assembly that merges the segmentation maps from the various subclass descriptors to ensure a more comprehensive representation of the different aspects in the test images. Through comprehensive experiments on three standard benchmarks, our method outperforms traditional text-supervised semantic segmentation methods by a marked margin.")],-1),Ch=e("h4",{id:"how-much-are-llms-contaminated-a-comprehensive-survey-and-the-llmsanitize-library",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#how-much-are-llms-contaminated-a-comprehensive-survey-and-the-llmsanitize-library"},[e("span",null,"How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library")])],-1),Sh=e("p",null,[e("strong",null,"Authors"),t(": Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty")],-1),Ih=e("strong",null,"Link",-1),zh={href:"http://arxiv.org/abs/2404.00699v1",target:"_blank",rel:"noopener noreferrer"},Eh=e("p",null,[e("strong",null,"Abstract"),t(": With the rise of Large Language Models (LLMs) in recent years, new opportunities are emerging, but also new challenges, and contamination is quickly becoming critical. Business applications and fundraising in AI have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a critical issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes the entire progress in the field of NLP, yet, there remains a lack of methods on how to efficiently address contamination, or a clear consensus on prevention, mitigation and classification of contamination. In this paper, we survey all recent work on contamination with LLMs, and help the community track contamination levels of LLMs by releasing an open-source Python library named LLMSanitize implementing major contamination detection algorithms, which link is: https://github.com/ntunlp/LLMSanitize.")],-1),qh=e("h4",{id:"llm-meets-vision-language-models-for-zero-shot-one-class-classification",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llm-meets-vision-language-models-for-zero-shot-one-class-classification"},[e("span",null,"LLM meets Vision-Language Models for Zero-Shot One-Class Classification")])],-1),Ph=e("p",null,[e("strong",null,"Authors"),t(": Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux, Vincent Gripon")],-1),Wh=e("strong",null,"Link",-1),Gh={href:"http://arxiv.org/abs/2404.00675v2",target:"_blank",rel:"noopener noreferrer"},Rh=e("p",null,[e("strong",null,"Abstract"),t(": We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to discriminate between a single category and other semantically related ones using only its label")],-1),Hh=e("h4",{id:"face-it-yourselves-an-llm-based-two-stage-strategy-to-localize-configuration-errors-via-logs",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#face-it-yourselves-an-llm-based-two-stage-strategy-to-localize-configuration-errors-via-logs"},[e("span",null,"Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs")])],-1),Bh=e("p",null,[e("strong",null,"Authors"),t(": Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, Zibin Zheng")],-1),Dh=e("strong",null,"Link",-1),Oh={href:"http://arxiv.org/abs/2404.00640v2",target:"_blank",rel:"noopener noreferrer"},Fh=e("p",null,[e("strong",null,"Abstract"),t(": Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis. To the best of our knowledge, this is the first work to localize the root-cause configuration properties for end-users based on Large Language Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by LogConfigLocalizer and prove its efficiency with an average accuracy as high as 99.91%. Additionally, we also demonstrate the effectiveness and necessity of different phases of the methodology by comparing it with two other variants and a baseline tool. Moreover, we validate the proposed methodology through a practical case study to demonstrate its effectiveness and feasibility.")],-1),jh=e("h4",{id:"ai-act-and-large-language-models-llms-when-critical-issues-and-privacy-impact-require-human-and-ethical-oversight",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ai-act-and-large-language-models-llms-when-critical-issues-and-privacy-impact-require-human-and-ethical-oversight"},[e("span",null,"AI Act and Large Language Models (LLMs): When critical issues and privacy impact require human and ethical oversight")])],-1),Kh=e("p",null,[e("strong",null,"Authors"),t(": Nicola Fabiano")],-1),Yh=e("strong",null,"Link",-1),Nh={href:"http://arxiv.org/abs/2404.00600v2",target:"_blank",rel:"noopener noreferrer"},Zh=e("p",null,[e("strong",null,"Abstract"),t(": The imposing evolution of artificial intelligence systems and, specifically, of Large Language Models (LLM) makes it necessary to carry out assessments of their level of risk and the impact they may have in the area of privacy, personal data protection and at an ethical level, especially on the weakest and most vulnerable. This contribution addresses human oversight, ethical oversight, and privacy impact assessment.")],-1),Jh=e("h4",{id:"chops-chat-with-customer-profile-systems-for-customer-service-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#chops-chat-with-customer-profile-systems-for-customer-service-with-llms"},[e("span",null,"CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs")])],-1),Vh=e("p",null,[e("strong",null,"Authors"),t(": Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li")],-1),Uh=e("strong",null,"Link",-1),Qh={href:"http://arxiv.org/abs/2404.01343v1",target:"_blank",rel:"noopener noreferrer"},Xh=e("p",null,[e("strong",null,"Abstract"),t(": Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and large LLMs to achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and QA pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how LLMs can enhance or serve as alternatives to human customer service. Our code and dataset will be open-sourced soon.")],-1),$h=e("h4",{id:"my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#my-agent-understands-me-better-integrating-dynamic-human-like-memory-recall-and-consolidation-in-llm-based-agents"},[e("span",null,'"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents')])],-1),ed=e("p",null,[e("strong",null,"Authors"),t(": Yuki Hou, Haruki Tamoto, Homei Miyashita")],-1),td=e("strong",null,"Link",-1),nd={href:"http://arxiv.org/abs/2404.00573v1",target:"_blank",rel:"noopener noreferrer"},ad=e("p",null,[e("strong",null,"Abstract"),t(": In this study, we propose a novel human-like memory architecture designed for enhancing the cognitive abilities of large language model based dialogue agents. Our proposed architecture enables agents to autonomously recall memories necessary for response generation, effectively addressing a limitation in the temporal cognition of LLMs. We adopt the human memory cue recall as a trigger for accurate and efficient memory recall. Moreover, we developed a mathematical model that dynamically quantifies memory consolidation, considering factors such as contextual relevance, elapsed time, and recall frequency. The agent stores memories retrieved from the user's interaction history in a database that encapsulates each memory's content and temporal context. Thus, this strategic storage allows agents to recall specific memories and understand their significance to the user in a temporal context, similar to how humans recognize and recall past experiences.")],-1),id=e("h4",{id:"divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#divtod-unleashing-the-power-of-llms-for-diversifying-task-oriented-dialogue-representations"},[e("span",null,"DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations")])],-1),od=e("p",null,[e("strong",null,"Authors"),t(": Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu")],-1),sd=e("strong",null,"Link",-1),rd={href:"http://arxiv.org/abs/2404.00557v1",target:"_blank",rel:"noopener noreferrer"},ld=e("p",null,[e("strong",null,"Abstract"),t(": Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.")],-1),cd=e("h4",{id:"llms-are-good-action-recognizers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-are-good-action-recognizers"},[e("span",null,"LLMs are Good Action Recognizers")])],-1),hd=e("p",null,[e("strong",null,"Authors"),t(": Haoxuan Qu, Yujun Cai, Jun Liu")],-1),dd=e("strong",null,"Link",-1),ud={href:"http://arxiv.org/abs/2404.00532v1",target:"_blank",rel:"noopener noreferrer"},gd=e("p",null,[e("strong",null,"Abstract"),t(": Skeleton-based action recognition has attracted lots of research attention. Recently, to build an accurate skeleton-based action recognizer, a variety of works have been proposed. Among them, some works use large model architectures as backbones of their recognizers to boost the skeleton data representation capability, while some other works pre-train their recognizers on external data to enrich the knowledge. In this work, we observe that large language models which have been extensively used in various natural language processing tasks generally hold both large model architectures and rich implicit knowledge. Motivated by this, we propose a novel LLM-AR framework, in which we investigate treating the Large Language Model as an Action Recognizer. In our framework, we propose a linguistic projection process to project each input action signal (i.e., each skeleton sequence) into its "),e("code",null,"sentence format'' (i.e., an "),t("action sentence''). Moreover, we also incorporate our framework with several designs to further facilitate this linguistic projection process. Extensive experiments demonstrate the efficacy of our proposed framework.")],-1),pd=e("h2",{id:"_2024-03-30",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-30"},[e("span",null,"2024-03-30")])],-1),md=e("h4",{id:"contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#contextual-ai-journaling-integrating-llm-and-time-series-behavioral-sensing-technology-to-promote-self-reflection-and-well-being-using-the-mindscape-app"},[e("span",null,"Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App")])],-1),fd=e("p",null,[e("strong",null,"Authors"),t(": Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp, Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell")],-1),vd=e("strong",null,"Link",-1),bd={href:"http://arxiv.org/abs/2404.00487v1",target:"_blank",rel:"noopener noreferrer"},yd=e("p",null,[e("strong",null,"Abstract"),t(": MindScape aims to study the benefits of integrating time series behavioral patterns (e.g., conversational engagement, sleep, location) with Large Language Models (LLMs) to create a new form of contextual AI journaling, promoting self-reflection and well-being. We argue that integrating behavioral sensing in LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work paper, we discuss the MindScape contextual journal App design that uses LLMs and behavioral sensing to generate contextual and personalized journaling prompts crafted to encourage self-reflection and emotional development. We also discuss the MindScape study of college students based on a preliminary user study and our upcoming study to assess the effectiveness of contextual AI journaling in promoting better well-being on college campuses. MindScape represents a new application class that embeds behavioral intelligence in AI.")],-1),Ld=e("h4",{id:"dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#dialectical-alignment-resolving-the-tension-of-3h-and-security-threats-of-llms"},[e("span",null,"Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs")])],-1),wd=e("p",null,[e("strong",null,"Authors"),t(": Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang")],-1),_d=e("strong",null,"Link",-1),kd={href:"http://arxiv.org/abs/2404.00486v1",target:"_blank",rel:"noopener noreferrer"},xd=e("p",null,[e("strong",null,"Abstract"),t(": With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of "),e("code",null,"you may be attacked"),t(" to the LLMs' context window.")],-1),Md=e("h4",{id:"numerologic-number-encoding-for-enhanced-llms-numerical-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#numerologic-number-encoding-for-enhanced-llms-numerical-reasoning"},[e("span",null,"NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning")])],-1),Ad=e("p",null,[e("strong",null,"Authors"),t(": Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, Assaf Arbelle")],-1),Td=e("strong",null,"Link",-1),Cd={href:"http://arxiv.org/abs/2404.00459v1",target:"_blank",rel:"noopener noreferrer"},Sd=e("p",null,[e("strong",null,"Abstract"),t(': Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of "42", we suggest using "{2:42}" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.')],-1),Id=e("h4",{id:"metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#metaie-distilling-a-meta-model-from-llm-for-all-kinds-of-information-extraction-tasks"},[e("span",null,"MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks")])],-1),zd=e("p",null,[e("strong",null,"Authors"),t(": Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang")],-1),Ed=e("strong",null,"Link",-1),qd={href:"http://arxiv.org/abs/2404.00457v1",target:"_blank",rel:"noopener noreferrer"},Pd=e("p",null,[e("strong",null,"Abstract"),t(': Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of "important information". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.')],-1),Wd=e("h4",{id:"quarot-outlier-free-4-bit-inference-in-rotated-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#quarot-outlier-free-4-bit-inference-in-rotated-llms"},[e("span",null,"QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs")])],-1),Gd=e("p",null,[e("strong",null,"Authors"),t(": Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman")],-1),Rd=e("strong",null,"Link",-1),Hd={href:"http://arxiv.org/abs/2404.00456v1",target:"_blank",rel:"noopener noreferrer"},Bd=e("p",null,[e("strong",null,"Abstract"),t(": We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.")],-1),Dd=e("h4",{id:"a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration"},[e("span",null,"A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration")])],-1),Od=e("p",null,[e("strong",null,"Authors"),t(": Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, Thomas W. Malone")],-1),Fd=e("strong",null,"Link",-1),jd={href:"http://arxiv.org/abs/2404.00405v1",target:"_blank",rel:"noopener noreferrer"},Kd=e("p",null,[e("strong",null,"Abstract"),t(`: With ChatGPT's release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the "5W1H" guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.`)],-1),Yd=e("h4",{id:"can-llms-master-math-investigating-large-language-models-on-math-stack-exchange",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-master-math-investigating-large-language-models-on-math-stack-exchange"},[e("span",null,"Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange")])],-1),Nd=e("p",null,[e("strong",null,"Authors"),t(": Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp")],-1),Zd=e("strong",null,"Link",-1),Jd={href:"http://arxiv.org/abs/2404.00344v1",target:"_blank",rel:"noopener noreferrer"},Vd=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}")],-1),Ud=e("h4",{id:"augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#augmenting-ner-datasets-with-llms-towards-automated-and-refined-annotation"},[e("span",null,"Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation")])],-1),Qd=e("p",null,[e("strong",null,"Authors"),t(": Yuji Naraki, Ryosuke Yamaki, Yoshikazu Ikeda, Takafumi Horie, Hiroki Naganuma")],-1),Xd=e("strong",null,"Link",-1),$d={href:"http://arxiv.org/abs/2404.01334v1",target:"_blank",rel:"noopener noreferrer"},eu=e("p",null,[e("strong",null,"Abstract"),t(": In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under constrained budget conditions. This study illuminates the potential of leveraging LLMs to improve dataset quality, introduces a novel technique to mitigate class imbalances, and demonstrates the feasibility of achieving high-performance NER in a cost-effective way.")],-1),tu=e("h4",{id:"a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#a-comprehensive-study-on-nlp-data-augmentation-for-hate-speech-detection-legacy-methods-bert-and-llms"},[e("span",null,"A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs")])],-1),nu=e("p",null,[e("strong",null,"Authors"),t(": Md Saroar Jahan, Mourad Oussalah, Djamila Romaissa Beddia, Jhuma kabir Mim, Nabil Arhab")],-1),au=e("strong",null,"Link",-1),iu={href:"http://arxiv.org/abs/2404.00303v1",target:"_blank",rel:"noopener noreferrer"},ou=e("p",null,[e("strong",null,"Abstract"),t(": The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.")],-1),su=e("h4",{id:"secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#secret-keepers-the-impact-of-llms-on-linguistic-markers-of-personal-traits"},[e("span",null,"Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits")])],-1),ru=e("p",null,[e("strong",null,"Authors"),t(": Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan Wen, Ala Tak, Fred Morstatter, Morteza Dehghani")],-1),lu=e("strong",null,"Link",-1),cu={href:"http://arxiv.org/abs/2404.00267v1",target:"_blank",rel:"noopener noreferrer"},hu=e("p",null,[e("strong",null,"Abstract"),t(": Prior research has established associations between individuals' language usage and their personal traits; our linguistic patterns reveal information about our personalities, emotional states, and beliefs. However, with the increasing adoption of Large Language Models (LLMs) as writing assistants in everyday writing, a critical question emerges: are authors' linguistic patterns still predictive of their personal traits when LLMs are involved in the writing process? We investigate the impact of LLMs on the linguistic markers of demographic and psychological traits, specifically examining three LLMs - GPT3.5, Llama 2, and Gemini - across six different traits: gender, age, political affiliation, personality, empathy, and morality. Our findings indicate that although the use of LLMs slightly reduces the predictive power of linguistic patterns over authors' personal traits, the significant changes are infrequent, and the use of LLMs does not fully diminish the predictive power of authors' linguistic patterns over their personal traits. We also note that some theoretically established lexical-based linguistic markers lose their reliability as predictors when LLMs are used in the writing process. Our findings have important implications for the study of linguistic markers of personal traits in the age of LLMs.")],-1),du=e("h4",{id:"deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#deft-flash-tree-attention-with-io-awareness-for-efficient-tree-search-based-llm-inference"},[e("span",null,"DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference")])],-1),uu=e("p",null,[e("strong",null,"Authors"),t(": Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin")],-1),gu=e("strong",null,"Link",-1),pu={href:"http://arxiv.org/abs/2404.00242v1",target:"_blank",rel:"noopener noreferrer"},mu=e("p",null,[e("strong",null,"Abstract"),t(": Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculation: we calculate partial attention of each QKV groups in a fused kernel then apply a Tree-topology-aware Global Reduction strategy to get final attention. Thanks to a reduction in KV cache IO by 3.6-4.5$\\times$, along with an additional reduction in IO for $\\mathbf{Q} \\mathbf{K}^\\top$ and Softmax equivalent to 25% of the total KV cache IO, DeFT can achieve a speedup of 1.7-2.4$\\times$ in end-to-end latency across two practical reasoning tasks over the SOTA attention algorithms.")],-1),fu=e("h4",{id:"is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#is-factuality-decoding-a-free-lunch-for-llms-evaluation-on-knowledge-editing-benchmark"},[e("span",null,"Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark")])],-1),vu=e("p",null,[e("strong",null,"Authors"),t(": Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng")],-1),bu=e("strong",null,"Link",-1),yu={href:"http://arxiv.org/abs/2404.00216v1",target:"_blank",rel:"noopener noreferrer"},Lu=e("p",null,[e("strong",null,"Abstract"),t(": The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of preserving the flexibility for knowledge editing. Therefore, our work suggests that research into factual alignment should simultaneously focus on the effectiveness of knowledge editing.")],-1),wu=e("h2",{id:"_2024-03-29",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-29"},[e("span",null,"2024-03-29")])],-1),_u=e("h4",{id:"towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference"},[e("span",null,"Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference")])],-1),ku=e("p",null,[e("strong",null,"Authors"),t(": Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, Josep Torrellas")],-1),xu=e("strong",null,"Link",-1),Mu={href:"http://arxiv.org/abs/2403.20306v1",target:"_blank",rel:"noopener noreferrer"},Au=e("p",null,[e("strong",null,"Abstract"),t(": With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective LLM deployment in data center environments.")],-1),Tu=e("h4",{id:"can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain"},[e("span",null,"Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain")])],-1),Cu=e("p",null,[e("strong",null,"Authors"),t(": Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini")],-1),Su=e("strong",null,"Link",-1),Iu={href:"http://arxiv.org/abs/2403.20288v1",target:"_blank",rel:"noopener noreferrer"},zu=e("p",null,[e("strong",null,"Abstract"),t(": We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.")],-1),Eu=e("h4",{id:"luq-long-text-uncertainty-quantification-for-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#luq-long-text-uncertainty-quantification-for-llms"},[e("span",null,"LUQ: Long-text Uncertainty Quantification for LLMs")])],-1),qu=e("p",null,[e("strong",null,"Authors"),t(": Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier")],-1),Pu=e("strong",null,"Link",-1),Wu={href:"http://arxiv.org/abs/2403.20279v1",target:"_blank",rel:"noopener noreferrer"},Gu=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \\textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \\textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \\textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular LLMs' response confidence spectrum and how that interplays with the response' factuality. We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about. To further improve the factual accuracy of LLM responses, we propose a method called \\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.")],-1),Ru=e("h4",{id:"using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations"},[e("span",null,"Using LLMs to Model the Beliefs and Preferences of Targeted Populations")])],-1),Hu=e("p",null,[e("strong",null,"Authors"),t(": Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Arechiga")],-1),Bu=e("strong",null,"Link",-1),Du={href:"http://arxiv.org/abs/2403.20252v1",target:"_blank",rel:"noopener noreferrer"},Ou=e("p",null,[e("strong",null,"Abstract"),t(": We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.")],-1),Fu=e("h4",{id:"accurate-block-quantization-in-llms-with-outliers",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#accurate-block-quantization-in-llms-with-outliers"},[e("span",null,"Accurate Block Quantization in LLMs with Outliers")])],-1),ju=e("p",null,[e("strong",null,"Authors"),t(": Nikita Trukhanov, Ilya Soloveychik")],-1),Ku=e("strong",null,"Link",-1),Yu={href:"http://arxiv.org/abs/2403.20137v1",target:"_blank",rel:"noopener noreferrer"},Nu=e("p",null,[e("strong",null,"Abstract"),t(": The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and provide extremely good quantization accuracy. The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block. In this paper, we focus on the most critical problem of limited KV-cache storage. We propose a novel approach enabling usage of low precision BFP formats without compromising the resulting model accuracy. We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their quantization quality is significantly improved. The methodology yields 2x savings in the memory footprint without significant degradation of the model's accuracy. Importantly, the rearrangement of channels happens at the compile time and thus has no impact on the inference latency.")],-1),Zu=e("h4",{id:"can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning"},[e("span",null,"Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning")])],-1),Ju=e("p",null,[e("strong",null,"Authors"),t(": Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang")],-1),Vu=e("strong",null,"Link",-1),Uu={href:"http://arxiv.org/abs/2403.20046v1",target:"_blank",rel:"noopener noreferrer"},Qu=e("p",null,[e("strong",null,"Abstract"),t(": Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \\textsc{CoTErrorSet} will be published soon on \\texttt{Anonymity Link}.")],-1),Xu=e("h4",{id:"enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning"},[e("span",null,"Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning")])],-1),$u=e("p",null,[e("strong",null,"Authors"),t(": Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li")],-1),eg=e("strong",null,"Link",-1),tg={href:"http://arxiv.org/abs/2403.19962v1",target:"_blank",rel:"noopener noreferrer"},ng=e("p",null,[e("strong",null,"Abstract"),t(": Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.")],-1),ag=e("h4",{id:"are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching"},[e("span",null,"Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching")])],-1),ig=e("p",null,[e("strong",null,"Authors"),t(": Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang")],-1),og=e("strong",null,"Link",-1),sg={href:"http://arxiv.org/abs/2403.19930v1",target:"_blank",rel:"noopener noreferrer"},rg=e("p",null,[e("strong",null,"Abstract"),t(": The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats.")],-1),lg=e("h2",{id:"_2024-03-28",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-28"},[e("span",null,"2024-03-28")])],-1),cg=e("h4",{id:"i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#i-m-categorizing-llm-as-a-productivity-tool-examining-ethics-of-llm-use-in-hci-research-practices"},[e("span",null,`"I'm categorizing LLM as a productivity tool": Examining ethics of LLM use in HCI research practices`)])],-1),hg=e("p",null,[e("strong",null,"Authors"),t(": Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen")],-1),dg=e("strong",null,"Link",-1),ug={href:"http://arxiv.org/abs/2403.19876v1",target:"_blank",rel:"noopener noreferrer"},gg=e("p",null,[e("strong",null,"Abstract"),t(": Large language models are increasingly applied in real-world scenarios, including research and education. These models, however, come with well-known ethical issues, which may manifest in unexpected ways in human-computer interaction research due to the extensive engagement with human subjects. This paper reports on research practices related to LLM use, drawing on 16 semi-structured interviews and a survey conducted with 50 HCI researchers. We discuss the ways in which LLMs are already being utilized throughout the entire HCI research pipeline, from ideation to system development and paper writing. While researchers described nuanced understandings of ethical issues, they were rarely or only partially able to identify and address those ethical concerns in their own projects. This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities. Finally, we reflect on the implications of our findings and present opportunities to shape emerging norms of engaging with large language models in HCI research.")],-1),pg=e("h4",{id:"llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llmsense-harnessing-llms-for-high-level-reasoning-over-spatiotemporal-sensor-traces"},[e("span",null,"LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces")])],-1),mg=e("p",null,[e("strong",null,"Authors"),t(": Xiaomin Ouyang, Mani Srivastava")],-1),fg=e("strong",null,"Link",-1),vg={href:"http://arxiv.org/abs/2403.19857v1",target:"_blank",rel:"noopener noreferrer"},bg=e("p",null,[e("strong",null,"Abstract"),t(": Most studies on machine learning in sensing systems focus on low-level perception tasks that process raw sensory data within a short time window. However, many practical applications, such as human routine modeling and occupancy tracking, require high-level reasoning abilities to comprehend concepts and make inferences based on long-term sensor traces. Existing machine learning-based approaches for handling such complex tasks struggle to generalize due to the limited training samples and the high dimensionality of sensor traces, necessitating the integration of human knowledge for designing first-principle models or logic reasoning methods. We pose a fundamental question: Can we harness the reasoning capabilities and world knowledge of Large Language Models (LLMs) to recognize complex events from long-term spatiotemporal sensor traces? To answer this question, we design an effective prompting framework for LLMs on high-level reasoning tasks, which can handle traces from the raw sensor data as well as the low-level perception results. We also design two strategies to enhance performance with long sensor traces, including summarization before reasoning and selective inclusion of historical traces. Our framework can be implemented in an edge-cloud setup, running small LLMs on the edge for data summarization and performing high-level reasoning on the cloud for privacy preservation. The results show that LLMSense can achieve over 80% accuracy on two high-level reasoning tasks such as dementia diagnosis with behavior traces and occupancy tracking with environmental sensor traces. This paper provides a few insights and guidelines for leveraging LLM for high-level reasoning on sensor traces and highlights several directions for future work.")],-1),yg=e("h4",{id:"llms-as-academic-reading-companions-extending-hci-through-synthetic-personae",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-as-academic-reading-companions-extending-hci-through-synthetic-personae"},[e("span",null,"LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae")])],-1),Lg=e("p",null,[e("strong",null,"Authors"),t(": Celia Chen, Alex Leitch")],-1),wg=e("strong",null,"Link",-1),_g={href:"http://arxiv.org/abs/2403.19506v1",target:"_blank",rel:"noopener noreferrer"},kg=e("p",null,[e("strong",null,"Abstract"),t(": This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.")],-1),xg=e("h4",{id:"enhancing-anomaly-detection-in-financial-markets-with-an-llm-based-multi-agent-framework",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-anomaly-detection-in-financial-markets-with-an-llm-based-multi-agent-framework"},[e("span",null,"Enhancing Anomaly Detection in Financial Markets with an LLM-based Multi-Agent Framework")])],-1),Mg=e("p",null,[e("strong",null,"Authors"),t(": Taejin Park")],-1),Ag=e("strong",null,"Link",-1),Tg={href:"http://arxiv.org/abs/2403.19735v1",target:"_blank",rel:"noopener noreferrer"},Cg=e("p",null,[e("strong",null,"Abstract"),t(": This paper introduces a Large Language Model (LLM)-based multi-agent framework designed to enhance anomaly detection within financial market data, tackling the longstanding challenge of manually verifying system-generated anomaly alerts. The framework harnesses a collaborative network of AI agents, each specialised in distinct functions including data conversion, expert analysis via web research, institutional knowledge utilization or cross-checking and report consolidation and management roles. By coordinating these agents towards a common objective, the framework provides a comprehensive and automated approach for validating and interpreting financial data anomalies. I analyse the S&P 500 index to demonstrate the framework's proficiency in enhancing the efficiency, accuracy and reduction of human intervention in financial market monitoring. The integration of AI's autonomous functionalities with established analytical methods not only underscores the framework's effectiveness in anomaly detection but also signals its broader applicability in supporting financial market monitoring.")],-1),Sg=e("h4",{id:"checkpoint-merging-via-bayesian-optimization-in-llm-pretraining",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#checkpoint-merging-via-bayesian-optimization-in-llm-pretraining"},[e("span",null,"Checkpoint Merging via Bayesian Optimization in LLM Pretraining")])],-1),Ig=e("p",null,[e("strong",null,"Authors"),t(": Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui")],-1),zg=e("strong",null,"Link",-1),Eg={href:"http://arxiv.org/abs/2403.19390v1",target:"_blank",rel:"noopener noreferrer"},qg=e("p",null,[e("strong",null,"Abstract"),t(": The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.")],-1),Pg=e("h4",{id:"breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#breaking-the-length-barrier-llm-enhanced-ctr-prediction-in-long-textual-user-behaviors"},[e("span",null,"Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors")])],-1),Wg=e("p",null,[e("strong",null,"Authors"),t(": Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, Linjian Mo")],-1),Gg=e("strong",null,"Link",-1),Rg={href:"http://arxiv.org/abs/2403.19347v1",target:"_blank",rel:"noopener noreferrer"},Hg=e("p",null,[e("strong",null,"Abstract"),t(": With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database. Subsequently, the deeper, trainable layers of the LLM facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings. This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity. Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores. Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using LLMs, especially with longer user sequences. BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR prediction.")],-1),Bg=e("h4",{id:"tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#tablellm-enabling-tabular-data-manipulation-by-llms-in-real-office-usage-scenarios"},[e("span",null,"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios")])],-1),Dg=e("p",null,[e("strong",null,"Authors"),t(": Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, Jie Tang")],-1),Og=e("strong",null,"Link",-1),Fg={href:"http://arxiv.org/abs/2403.19318v1",target:"_blank",rel:"noopener noreferrer"},jg=e("p",null,[e("strong",null,"Abstract"),t(": We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction.")],-1),Kg=e("h4",{id:"generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#generate-then-retrieve-conversational-response-retrieval-using-llms-as-answer-and-query-generators"},[e("span",null,"Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators")])],-1),Yg=e("p",null,[e("strong",null,"Authors"),t(": Zahra Abbasiantaeb, Mohammad Aliannejadi")],-1),Ng=e("strong",null,"Link",-1),Zg={href:"http://arxiv.org/abs/2403.19302v1",target:"_blank",rel:"noopener noreferrer"},Jg=e("p",null,[e("strong",null,"Abstract"),t(": CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset.")],-1),Vg=e("h4",{id:"top-leaderboard-ranking-top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#top-leaderboard-ranking-top-coding-proficiency-always-evoeval-evolving-coding-benchmarks-via-llm"},[e("span",null,"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM")])],-1),Ug=e("p",null,[e("strong",null,"Authors"),t(": Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang")],-1),Qg=e("strong",null,"Link",-1),Xg={href:"http://arxiv.org/abs/2403.19114v1",target:"_blank",rel:"noopener noreferrer"},$g=e("p",null,[e("strong",null,"Abstract"),t(": LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval")],-1),ep=e("h4",{id:"learning-from-correctness-without-prompting-makes-llm-efficient-reasoner",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-from-correctness-without-prompting-makes-llm-efficient-reasoner"},[e("span",null,"Learning From Correctness Without Prompting Makes LLM Efficient Reasoner")])],-1),tp=e("p",null,[e("strong",null,"Authors"),t(": Yuxuan Yao, Han Wu, Zhijiang Guo, Biyan Zhou, Jiahui Gao, Sichun Luo, Hanxu Hou, Xiaojin Fu, Linqi Song")],-1),np=e("strong",null,"Link",-1),ap={href:"http://arxiv.org/abs/2403.19094v1",target:"_blank",rel:"noopener noreferrer"},ip=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption.")],-1),op=e("h2",{id:"_2024-03-27",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-27"},[e("span",null,"2024-03-27")])],-1),sp=e("h4",{id:"towards-llm-recsys-alignment-with-textual-id-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-llm-recsys-alignment-with-textual-id-learning"},[e("span",null,"Towards LLM-RecSys Alignment with Textual ID Learning")])],-1),rp=e("p",null,[e("strong",null,"Authors"),t(": Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang")],-1),lp=e("strong",null,"Link",-1),cp={href:"http://arxiv.org/abs/2403.19021v1",target:"_blank",rel:"noopener noreferrer"},hp=e("p",null,[e("strong",null,"Abstract"),t(": Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.")],-1),dp=e("h4",{id:"physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#physicsassistant-an-llm-powered-interactive-learning-robot-for-physics-lab-investigations"},[e("span",null,"PhysicsAssistant: An LLM-Powered Interactive Learning Robot for Physics Lab Investigations")])],-1),up=e("p",null,[e("strong",null,"Authors"),t(": Ehsan Latif, Ramviyas Parasuraman, Xiaoming Zhai")],-1),gp=e("strong",null,"Link",-1),pp={href:"http://arxiv.org/abs/2403.18721v1",target:"_blank",rel:"noopener noreferrer"},mp=e("p",null,[e("strong",null,"Abstract"),t(": Robot systems in education can leverage Large language models' (LLMs) natural language understanding capabilities to provide assistance and facilitate learning. This paper proposes a multimodal interactive robot (PhysicsAssistant) built on YOLOv8 object detection, cameras, speech recognition, and chatbot using LLM to provide assistance to students' physics labs. We conduct a user study on ten 8th-grade students to empirically evaluate the performance of PhysicsAssistant with a human expert. The Expert rates the assistants' responses to student queries on a 0-4 scale based on Bloom's taxonomy to provide educational support. We have compared the performance of PhysicsAssistant (YOLOv8+GPT-3.5-turbo) with GPT-4 and found that the human expert rating of both systems for factual understanding is the same. However, the rating of GPT-4 for conceptual and procedural knowledge (3 and 3.2 vs 2.2 and 2.6, respectively) is significantly higher than PhysicsAssistant (p < 0.05). However, the response time of GPT-4 is significantly higher than PhysicsAssistant (3.54 vs 1.64 sec, p < 0.05). Hence, despite the relatively lower response quality of PhysicsAssistant than GPT-4, it has shown potential for being used as a real-time lab assistant to provide timely responses and can offload teachers' labor to assist with repetitive tasks. To the best of our knowledge, this is the first attempt to build such an interactive multimodal robotic assistant for K-12 science (physics) education.")],-1),fp=e("h4",{id:"sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sdsat-accelerating-llm-inference-through-speculative-decoding-with-semantic-adaptive-tokens"},[e("span",null,"SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens")])],-1),vp=e("p",null,[e("strong",null,"Authors"),t(": Chengbo Liu, Yong Zhu")],-1),bp=e("strong",null,"Link",-1),yp={href:"http://arxiv.org/abs/2403.18647v1",target:"_blank",rel:"noopener noreferrer"},Lp=e("p",null,[e("strong",null,"Abstract"),t(`: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Please refer to https://github.com/hasuoshenyun/SDSAT.`)],-1),wp=e("h4",{id:"foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#foc-figure-out-the-cryptographic-functions-in-stripped-binaries-with-llms"},[e("span",null,"FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs")])],-1),_p=e("p",null,[e("strong",null,"Authors"),t(": Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Yanming Zhang, Weiming Zhang, Nenghai Yu")],-1),kp=e("strong",null,"Link",-1),xp={href:"http://arxiv.org/abs/2403.18403v1",target:"_blank",rel:"noopener noreferrer"},Mp=e("p",null,[e("strong",null,"Abstract"),t(": Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoCBinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.")],-1),Ap=e("h4",{id:"rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#rejection-improves-reliability-training-llms-to-refuse-unknown-questions-using-rl-from-knowledge-feedback"},[e("span",null,"Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback")])],-1),Tp=e("p",null,[e("strong",null,"Authors"),t(": Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu")],-1),Cp=e("strong",null,"Link",-1),Sp={href:"http://arxiv.org/abs/2403.18349v1",target:"_blank",rel:"noopener noreferrer"},Ip=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.")],-1),zp=e("h4",{id:"can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-converse-formally-automatically-assessing-llms-in-translating-and-interpreting-formal-specifications"},[e("span",null,"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications")])],-1),Ep=e("p",null,[e("strong",null,"Authors"),t(": Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava")],-1),qp=e("strong",null,"Link",-1),Pp={href:"http://arxiv.org/abs/2403.18327v1",target:"_blank",rel:"noopener noreferrer"},Wp=e("p",null,[e("strong",null,"Abstract"),t(": Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task and show that SOTA LLMs cannot adequately solve this task, limiting their current utility in the design of complex systems.")],-1),Gp=e("h4",{id:"exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-the-deceptive-power-of-llm-generated-fake-news-a-study-of-real-world-detection-challenges"},[e("span",null,"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges")])],-1),Rp=e("p",null,[e("strong",null,"Authors"),t(": Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu")],-1),Hp=e("strong",null,"Link",-1),Bp={href:"http://arxiv.org/abs/2403.18249v1",target:"_blank",rel:"noopener noreferrer"},Dp=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPrompt attacks, we created a new dataset named VLPrompt fake news (VLPFN) containing real and fake texts. Our experiments, including various detection methods and novel human study metrics, were conducted to assess their performance on our dataset, yielding numerous findings.")],-1),Op=e("h4",{id:"llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-in-hci-data-work-bridging-the-gap-between-information-retrieval-and-responsible-research-practices"},[e("span",null,"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval and Responsible Research Practices")])],-1),Fp=e("p",null,[e("strong",null,"Authors"),t(": Neda Taghizadeh Serajeh, Iman Mohammadi, Vittorio Fuccella, Mattia De Rosa")],-1),jp=e("strong",null,"Link",-1),Kp={href:"http://arxiv.org/abs/2403.18173v1",target:"_blank",rel:"noopener noreferrer"},Yp=e("p",null,[e("strong",null,"Abstract"),t(": Efficient and accurate information extraction from scientific papers is significant in the rapidly developing human-computer interaction research in the literature review process. Our paper introduces and analyses a new information retrieval system using state-of-the-art Large Language Models (LLMs) in combination with structured text analysis techniques to extract experimental data from HCI literature, emphasizing key elements. Then We analyze the challenges and risks of using LLMs in the world of research. We performed a comprehensive analysis on our conducted dataset, which contained the specified information of 300 CHI 2020-2022 papers, to evaluate the performance of the two large language models, GPT-3.5 (text-davinci-003) and Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model gains an accuracy of 58% and a mean absolute error of 7.00. In contrast, the Llama2 model indicates an accuracy of 56% with a mean absolute error of 7.63. The ability to answer questions was also included in the system in order to work with streamlined data. By evaluating the risks and opportunities presented by LLMs, our work contributes to the ongoing dialogue on establishing methodological validity and ethical guidelines for LLM use in HCI data work.")],-1),Np=e("h2",{id:"_2024-03-26",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-03-26"},[e("span",null,"2024-03-26")])],-1),Zp=e("h4",{id:"don-t-trust-verify-grounding-llm-quantitative-reasoning-with-autoformalization",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#don-t-trust-verify-grounding-llm-quantitative-reasoning-with-autoformalization"},[e("span",null,"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization")])],-1),Jp=e("p",null,[e("strong",null,"Authors"),t(": Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu")],-1),Vp=e("strong",null,"Link",-1),Up={href:"http://arxiv.org/abs/2403.18120v1",target:"_blank",rel:"noopener noreferrer"},Qp=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting -- the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes. The code can be found at https://github.com/jinpz/dtv.")],-1),Xp=e("h4",{id:"magis-llm-based-multi-agent-framework-for-github-issue-resolution",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#magis-llm-based-multi-agent-framework-for-github-issue-resolution"},[e("span",null,"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution")])],-1),$p=e("p",null,[e("strong",null,"Authors"),t(": Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng")],-1),em=e("strong",null,"Link",-1),tm={href:"http://arxiv.org/abs/2403.17927v1",target:"_blank",rel:"noopener noreferrer"},nm=e("p",null,[e("strong",null,"Abstract"),t(": In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.")],-1),am=e("h4",{id:"exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-llms-as-a-source-of-targeted-synthetic-textual-data-to-minimize-high-confidence-misclassifications"},[e("span",null,"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications")])],-1),im=e("p",null,[e("strong",null,"Authors"),t(": Philip Lippmann, Matthijs Spaan, Jie Yang")],-1),om=e("strong",null,"Link",-1),sm={href:"http://arxiv.org/abs/2403.17860v1",target:"_blank",rel:"noopener noreferrer"},rm=e("p",null,[e("strong",null,"Abstract"),t(": Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.")],-1),lm=e("h4",{id:"verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#verbing-weirds-language-models-evaluation-of-english-zero-derivation-in-five-llms"},[e("span",null,"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs")])],-1),cm=e("p",null,[e("strong",null,"Authors"),t(": David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler")],-1),hm=e("strong",null,"Link",-1),dm={href:"http://arxiv.org/abs/2403.17856v1",target:"_blank",rel:"noopener noreferrer"},um=e("p",null,[e("strong",null,"Abstract"),t(": Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4.")],-1),gm=e("h4",{id:"accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#accelerating-radio-spectrum-regulation-workflows-with-large-language-models-llms"},[e("span",null,"Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)")])],-1),pm=e("p",null,[e("strong",null,"Authors"),t(": Amir Ghasemi, Paul Guinand")],-1),mm=e("strong",null,"Link",-1),fm={href:"http://arxiv.org/abs/2403.17819v1",target:"_blank",rel:"noopener noreferrer"},vm=e("p",null,[e("strong",null,"Abstract"),t(": Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks. In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.")],-1);function bm(ym,Lm){const n=o("ExternalLinkIcon");return s(),r("div",null,[h,e("p",null,[d,t(": "),e("a",u,[t("http://arxiv.org/abs/2404.07926v1"),a(n)])]),g,p,m,e("p",null,[f,t(": "),e("a",v,[t("http://arxiv.org/abs/2404.07921v1"),a(n)])]),b,y,L,e("p",null,[w,t(": "),e("a",_,[t("http://arxiv.org/abs/2404.07677v1"),a(n)])]),k,x,M,e("p",null,[A,t(": "),e("a",T,[t("http://arxiv.org/abs/2404.07613v1"),a(n)])]),C,S,I,e("p",null,[z,t(": "),e("a",E,[t("http://arxiv.org/abs/2404.07584v1"),a(n)])]),q,P,W,e("p",null,[G,t(": "),e("a",R,[t("http://arxiv.org/abs/2404.07546v1"),a(n)])]),H,B,D,e("p",null,[O,t(": "),e("a",F,[t("http://arxiv.org/abs/2404.07456v1"),a(n)])]),j,K,Y,e("p",null,[N,t(": "),e("a",Z,[t("http://arxiv.org/abs/2404.07449v1"),a(n)])]),J,V,U,Q,e("p",null,[X,t(": "),e("a",$,[t("http://arxiv.org/abs/2404.07387v1"),a(n)])]),ee,te,ne,e("p",null,[ae,t(": "),e("a",ie,[t("http://arxiv.org/abs/2404.07382v1"),a(n)])]),oe,se,re,e("p",null,[le,t(": "),e("a",ce,[t("http://arxiv.org/abs/2404.07376v1"),a(n)])]),he,de,ue,e("p",null,[ge,t(": "),e("a",pe,[t("http://arxiv.org/abs/2404.07108v2"),a(n)])]),me,fe,ve,e("p",null,[be,t(": "),e("a",ye,[t("http://arxiv.org/abs/2404.06948v2"),a(n)])]),Le,we,_e,e("p",null,[ke,t(": "),e("a",xe,[t("http://arxiv.org/abs/2404.06921v1"),a(n)])]),Me,Ae,Te,e("p",null,[Ce,t(": "),e("a",Se,[t("http://arxiv.org/abs/2404.06838v1"),a(n)])]),Ie,ze,Ee,e("p",null,[qe,t(": "),e("a",Pe,[t("http://arxiv.org/abs/2404.06833v1"),a(n)])]),We,Ge,Re,e("p",null,[He,t(": "),e("a",Be,[t("http://arxiv.org/abs/2404.06809v1"),a(n)])]),De,Oe,Fe,e("p",null,[je,t(": "),e("a",Ke,[t("http://arxiv.org/abs/2404.06711v1"),a(n)])]),Ye,Ne,Ze,e("p",null,[Je,t(": "),e("a",Ve,[t("http://arxiv.org/abs/2404.06664v1"),a(n)])]),Ue,Qe,Xe,$e,e("p",null,[et,t(": "),e("a",tt,[t("http://arxiv.org/abs/2404.06644v1"),a(n)])]),nt,at,it,e("p",null,[ot,t(": "),e("a",st,[t("http://arxiv.org/abs/2404.07242v1"),a(n)])]),rt,lt,ct,e("p",null,[ht,t(": "),e("a",dt,[t("http://arxiv.org/abs/2404.06503v1"),a(n)])]),ut,gt,pt,e("p",null,[mt,t(": "),e("a",ft,[t("http://arxiv.org/abs/2404.06488v1"),a(n)])]),vt,bt,yt,e("p",null,[Lt,t(": "),e("a",wt,[t("http://arxiv.org/abs/2404.06480v1"),a(n)])]),_t,kt,xt,e("p",null,[Mt,t(": "),e("a",At,[t("http://arxiv.org/abs/2404.06411v1"),a(n)])]),Tt,Ct,St,e("p",null,[It,t(": "),e("a",zt,[t("http://arxiv.org/abs/2404.06371v1"),a(n)])]),Et,qt,Pt,e("p",null,[Wt,t(": "),e("a",Gt,[t("http://arxiv.org/abs/2404.06283v1"),a(n)])]),Rt,Ht,Bt,e("p",null,[Dt,t(": "),e("a",Ot,[t("http://arxiv.org/abs/2404.06082v1"),a(n)])]),Ft,jt,Kt,e("p",null,[Yt,t(": "),e("a",Nt,[t("http://arxiv.org/abs/2404.06041v1"),a(n)])]),Zt,Jt,Vt,e("p",null,[Ut,t(": "),e("a",Qt,[t("http://arxiv.org/abs/2404.06035v1"),a(n)])]),Xt,$t,en,e("p",null,[tn,t(": "),e("a",nn,[t("http://arxiv.org/abs/2404.05993v1"),a(n)])]),an,on,sn,e("p",null,[rn,t(": "),e("a",ln,[t("http://arxiv.org/abs/2404.05955v1"),a(n)])]),cn,hn,dn,un,e("p",null,[gn,t(": "),e("a",pn,[t("http://arxiv.org/abs/2404.05825v1"),a(n)])]),mn,fn,vn,e("p",null,[bn,t(": "),e("a",yn,[t("http://arxiv.org/abs/2404.05719v1"),a(n)])]),Ln,wn,_n,e("p",null,[kn,t(": "),e("a",xn,[t("http://arxiv.org/abs/2404.05674v1"),a(n)])]),Mn,An,Tn,e("p",null,[Cn,t(": "),e("a",Sn,[t("http://arxiv.org/abs/2404.05520v2"),a(n)])]),In,zn,En,e("p",null,[qn,t(": "),e("a",Pn,[t("http://arxiv.org/abs/2404.05502v1"),a(n)])]),Wn,Gn,Rn,e("p",null,[Hn,t(": "),e("a",Bn,[t("http://arxiv.org/abs/2404.05483v1"),a(n)])]),Dn,On,Fn,e("p",null,[jn,t(": "),e("a",Kn,[t("http://arxiv.org/abs/2404.05221v1"),a(n)])]),Yn,Nn,Zn,e("p",null,[Jn,t(": "),e("a",Vn,[t("http://arxiv.org/abs/2404.05213v1"),a(n)])]),Un,Qn,Xn,e("p",null,[$n,t(": "),e("a",ea,[t("http://arxiv.org/abs/2404.05183v1"),a(n)])]),ta,na,aa,e("p",null,[ia,t(": "),e("a",oa,[t("http://arxiv.org/abs/2404.05144v1"),a(n)])]),sa,ra,la,ca,e("p",null,[ha,t(": "),e("a",da,[t("http://arxiv.org/abs/2404.05044v1"),a(n)])]),ua,ga,pa,e("p",null,[ma,t(": "),e("a",fa,[t("http://arxiv.org/abs/2404.04997v1"),a(n)])]),va,ba,ya,e("p",null,[La,t(": "),e("a",wa,[t("http://arxiv.org/abs/2404.04966v1"),a(n)])]),_a,ka,xa,e("p",null,[Ma,t(": "),e("a",Aa,[t("http://arxiv.org/abs/2404.04902v1"),a(n)])]),Ta,Ca,Sa,e("p",null,[Ia,t(": "),e("a",za,[t("http://arxiv.org/abs/2404.04869v1"),a(n)])]),Ea,qa,Pa,e("p",null,[Wa,t(": "),e("a",Ga,[t("http://arxiv.org/abs/2404.07235v1"),a(n)])]),Ra,Ha,Ba,e("p",null,[Da,t(": "),e("a",Oa,[t("http://arxiv.org/abs/2404.04834v1"),a(n)])]),Fa,ja,Ka,e("p",null,[Ya,t(": "),e("a",Na,[t("http://arxiv.org/abs/2404.04809v1"),a(n)])]),Za,Ja,Va,e("p",null,[Ua,t(": "),e("a",Qa,[t("http://arxiv.org/abs/2404.04793v1"),a(n)])]),Xa,$a,ei,ti,e("p",null,[ni,t(": "),e("a",ai,[t("http://arxiv.org/abs/2404.04689v1"),a(n)])]),ii,oi,si,e("p",null,[ri,t(": "),e("a",li,[t("http://arxiv.org/abs/2404.04631v1"),a(n)])]),ci,hi,di,e("p",null,[ui,t(": "),e("a",gi,[t("http://arxiv.org/abs/2404.04603v1"),a(n)])]),pi,mi,fi,e("p",null,[vi,t(": "),e("a",bi,[t("http://arxiv.org/abs/2404.04570v1"),a(n)])]),yi,Li,wi,e("p",null,[_i,t(": "),e("a",ki,[t("http://arxiv.org/abs/2404.04510v1"),a(n)])]),xi,Mi,Ai,Ti,e("p",null,[Ci,t(": "),e("a",Si,[t("http://arxiv.org/abs/2404.04392v1"),a(n)])]),Ii,zi,Ei,e("p",null,[qi,t(": "),e("a",Pi,[t("http://arxiv.org/abs/2404.04376v1"),a(n)])]),Wi,Gi,Ri,e("p",null,[Hi,t(": "),e("a",Bi,[t("http://arxiv.org/abs/2404.04346v1"),a(n)])]),Di,Oi,Fi,e("p",null,[ji,t(": "),e("a",Ki,[t("http://arxiv.org/abs/2404.04167v1"),a(n)])]),Yi,Ni,Zi,e("p",null,[Ji,t(": "),e("a",Vi,[t("http://arxiv.org/abs/2404.04102v1"),a(n)])]),Ui,Qi,Xi,e("p",null,[$i,t(": "),e("a",eo,[t("http://arxiv.org/abs/2404.04067v1"),a(n)])]),to,no,ao,e("p",null,[io,t(": "),e("a",oo,[t("http://arxiv.org/abs/2404.04066v1"),a(n)])]),so,ro,lo,e("p",null,[co,t(": "),e("a",ho,[t("http://arxiv.org/abs/2404.03891v1"),a(n)])]),uo,go,po,e("p",null,[mo,t(": "),e("a",fo,[t("http://arxiv.org/abs/2404.03868v1"),a(n)])]),vo,bo,yo,Lo,e("p",null,[wo,t(": "),e("a",_o,[t("http://arxiv.org/abs/2404.04302v1"),a(n)])]),ko,xo,Mo,e("p",null,[Ao,t(": "),e("a",To,[t("http://arxiv.org/abs/2404.04298v1"),a(n)])]),Co,So,Io,e("p",null,[zo,t(": "),e("a",Eo,[t("http://arxiv.org/abs/2404.03746v1"),a(n)])]),qo,Po,Wo,e("p",null,[Go,t(": "),e("a",Ro,[t("http://arxiv.org/abs/2404.03745v1"),a(n)])]),Ho,Bo,Do,e("p",null,[Oo,t(": "),e("a",Fo,[t("http://arxiv.org/abs/2404.03732v1"),a(n)])]),jo,Ko,Yo,e("p",null,[No,t(": "),e("a",Zo,[t("http://arxiv.org/abs/2404.03626v1"),a(n)])]),Jo,Vo,Uo,e("p",null,[Qo,t(": "),e("a",Xo,[t("http://arxiv.org/abs/2404.03623v1"),a(n)])]),$o,es,ts,e("p",null,[ns,t(": "),e("a",as,[t("http://arxiv.org/abs/2404.03602v1"),a(n)])]),is,os,ss,e("p",null,[rs,t(": "),e("a",ls,[t("http://arxiv.org/abs/2404.03565v1"),a(n)])]),cs,hs,ds,e("p",null,[us,t(": "),e("a",gs,[t("http://arxiv.org/abs/2404.03413v1"),a(n)])]),ps,ms,fs,e("p",null,[vs,t(": "),e("a",bs,[t("http://arxiv.org/abs/2404.03192v1"),a(n)])]),ys,Ls,ws,e("p",null,[_s,t(": "),e("a",ks,[t("http://arxiv.org/abs/2404.03134v1"),a(n)])]),xs,Ms,As,e("p",null,[Ts,t(": "),e("a",Cs,[t("http://arxiv.org/abs/2404.03122v1"),a(n)])]),Ss,Is,zs,Es,e("p",null,[qs,t(": "),e("a",Ps,[t("http://arxiv.org/abs/2404.03044v1"),a(n)])]),Ws,Gs,Rs,e("p",null,[Hs,t(": "),e("a",Bs,[t("http://arxiv.org/abs/2404.02838v1"),a(n)])]),Ds,Os,Fs,e("p",null,[js,t(": "),e("a",Ks,[t("http://arxiv.org/abs/2404.02761v1"),a(n)])]),Ys,Ns,Zs,e("p",null,[Js,t(": "),e("a",Vs,[t("http://arxiv.org/abs/2404.02706v1"),a(n)])]),Us,Qs,Xs,e("p",null,[$s,t(": "),e("a",er,[t("http://arxiv.org/abs/2404.02616v1"),a(n)])]),tr,nr,ar,e("p",null,[ir,t(": "),e("a",or,[t("http://arxiv.org/abs/2404.02532v1"),a(n)])]),sr,rr,lr,e("p",null,[cr,t(": "),e("a",hr,[t("http://arxiv.org/abs/2404.02474v1"),a(n)])]),dr,ur,gr,e("p",null,[pr,t(": "),e("a",mr,[t("http://arxiv.org/abs/2404.02422v1"),a(n)])]),fr,vr,br,yr,e("p",null,[Lr,t(": "),e("a",wr,[t("http://arxiv.org/abs/2404.02294v1"),a(n)])]),_r,kr,xr,e("p",null,[Mr,t(": "),e("a",Ar,[t("http://arxiv.org/abs/2404.02261v1"),a(n)])]),Tr,Cr,Sr,e("p",null,[Ir,t(": "),e("a",zr,[t("http://arxiv.org/abs/2404.02151v1"),a(n)])]),Er,qr,Pr,e("p",null,[Wr,t(": "),e("a",Gr,[t("http://arxiv.org/abs/2404.02138v1"),a(n)])]),Rr,Hr,Br,e("p",null,[Dr,t(": "),e("a",Or,[t("http://arxiv.org/abs/2404.02078v1"),a(n)])]),Fr,jr,Kr,e("p",null,[Yr,t(": "),e("a",Nr,[t("http://arxiv.org/abs/2404.02060v1"),a(n)])]),Zr,Jr,Vr,e("p",null,[Ur,t(": "),e("a",Qr,[t("http://arxiv.org/abs/2404.02056v1"),a(n)])]),Xr,$r,el,e("p",null,[tl,t(": "),e("a",nl,[t("http://arxiv.org/abs/2404.02015v1"),a(n)])]),al,il,ol,e("p",null,[sl,t(": "),e("a",rl,[t("http://arxiv.org/abs/2404.02183v1"),a(n)])]),ll,cl,hl,e("p",null,[dl,t(": "),e("a",ul,[t("http://arxiv.org/abs/2404.01940v1"),a(n)])]),gl,pl,ml,e("p",null,[fl,t(": "),e("a",vl,[t("http://arxiv.org/abs/2404.01855v1"),a(n)])]),bl,yl,Ll,e("p",null,[wl,t(": "),e("a",_l,[t("http://arxiv.org/abs/2404.01833v1"),a(n)])]),kl,xl,Ml,e("p",null,[Al,t(": "),e("a",Tl,[t("http://arxiv.org/abs/2404.01616v2"),a(n)])]),Cl,Sl,Il,e("p",null,[zl,t(": "),e("a",El,[t("http://arxiv.org/abs/2404.01616v1"),a(n)])]),ql,Pl,Wl,Gl,e("p",null,[Rl,t(": "),e("a",Hl,[t("http://arxiv.org/abs/2404.01535v1"),a(n)])]),Bl,Dl,Ol,e("p",null,[Fl,t(": "),e("a",jl,[t("http://arxiv.org/abs/2404.01461v1"),a(n)])]),Kl,Yl,Nl,e("p",null,[Zl,t(": "),e("a",Jl,[t("http://arxiv.org/abs/2404.01453v1"),a(n)])]),Vl,Ul,Ql,e("p",null,[Xl,t(": "),e("a",$l,[t("http://arxiv.org/abs/2404.01430v1"),a(n)])]),ec,tc,nc,e("p",null,[ac,t(": "),e("a",ic,[t("http://arxiv.org/abs/2404.01425v1"),a(n)])]),oc,sc,rc,e("p",null,[lc,t(": "),e("a",cc,[t("http://arxiv.org/abs/2404.01365v1"),a(n)])]),hc,dc,uc,e("p",null,[gc,t(": "),e("a",pc,[t("http://arxiv.org/abs/2404.01268v1"),a(n)])]),mc,fc,vc,e("p",null,[bc,t(": "),e("a",yc,[t("http://arxiv.org/abs/2404.01230v1"),a(n)])]),Lc,wc,_c,e("p",null,[kc,t(": "),e("a",xc,[t("http://arxiv.org/abs/2404.01151v1"),a(n)])]),Mc,Ac,Tc,e("p",null,[Cc,t(": "),e("a",Sc,[t("http://arxiv.org/abs/2404.01147v1"),a(n)])]),Ic,zc,Ec,e("p",null,[qc,t(": "),e("a",Pc,[t("http://arxiv.org/abs/2404.01129v1"),a(n)])]),Wc,Gc,Rc,e("p",null,[Hc,t(": "),e("a",Bc,[t("http://arxiv.org/abs/2404.01361v1"),a(n)])]),Dc,Oc,Fc,e("p",null,[jc,t(": "),e("a",Kc,[t("http://arxiv.org/abs/2404.01096v1"),a(n)])]),Yc,Nc,Zc,e("p",null,[Jc,t(": "),e("a",Vc,[t("http://arxiv.org/abs/2404.01041v2"),a(n)])]),Uc,Qc,Xc,e("p",null,[$c,t(": "),e("a",eh,[t("http://arxiv.org/abs/2404.01353v1"),a(n)])]),th,nh,ah,e("p",null,[ih,t(": "),e("a",oh,[t("http://arxiv.org/abs/2404.00971v1"),a(n)])]),sh,rh,lh,e("p",null,[ch,t(": "),e("a",hh,[t("http://arxiv.org/abs/2404.00925v1"),a(n)])]),dh,uh,gh,e("p",null,[ph,t(": "),e("a",mh,[t("http://arxiv.org/abs/2404.00899v1"),a(n)])]),fh,vh,bh,yh,e("p",null,[Lh,t(": "),e("a",wh,[t("http://arxiv.org/abs/2404.00725v1"),a(n)])]),_h,kh,xh,e("p",null,[Mh,t(": "),e("a",Ah,[t("http://arxiv.org/abs/2404.00701v1"),a(n)])]),Th,Ch,Sh,e("p",null,[Ih,t(": "),e("a",zh,[t("http://arxiv.org/abs/2404.00699v1"),a(n)])]),Eh,qh,Ph,e("p",null,[Wh,t(": "),e("a",Gh,[t("http://arxiv.org/abs/2404.00675v2"),a(n)])]),Rh,Hh,Bh,e("p",null,[Dh,t(": "),e("a",Oh,[t("http://arxiv.org/abs/2404.00640v2"),a(n)])]),Fh,jh,Kh,e("p",null,[Yh,t(": "),e("a",Nh,[t("http://arxiv.org/abs/2404.00600v2"),a(n)])]),Zh,Jh,Vh,e("p",null,[Uh,t(": "),e("a",Qh,[t("http://arxiv.org/abs/2404.01343v1"),a(n)])]),Xh,$h,ed,e("p",null,[td,t(": "),e("a",nd,[t("http://arxiv.org/abs/2404.00573v1"),a(n)])]),ad,id,od,e("p",null,[sd,t(": "),e("a",rd,[t("http://arxiv.org/abs/2404.00557v1"),a(n)])]),ld,cd,hd,e("p",null,[dd,t(": "),e("a",ud,[t("http://arxiv.org/abs/2404.00532v1"),a(n)])]),gd,pd,md,fd,e("p",null,[vd,t(": "),e("a",bd,[t("http://arxiv.org/abs/2404.00487v1"),a(n)])]),yd,Ld,wd,e("p",null,[_d,t(": "),e("a",kd,[t("http://arxiv.org/abs/2404.00486v1"),a(n)])]),xd,Md,Ad,e("p",null,[Td,t(": "),e("a",Cd,[t("http://arxiv.org/abs/2404.00459v1"),a(n)])]),Sd,Id,zd,e("p",null,[Ed,t(": "),e("a",qd,[t("http://arxiv.org/abs/2404.00457v1"),a(n)])]),Pd,Wd,Gd,e("p",null,[Rd,t(": "),e("a",Hd,[t("http://arxiv.org/abs/2404.00456v1"),a(n)])]),Bd,Dd,Od,e("p",null,[Fd,t(": "),e("a",jd,[t("http://arxiv.org/abs/2404.00405v1"),a(n)])]),Kd,Yd,Nd,e("p",null,[Zd,t(": "),e("a",Jd,[t("http://arxiv.org/abs/2404.00344v1"),a(n)])]),Vd,Ud,Qd,e("p",null,[Xd,t(": "),e("a",$d,[t("http://arxiv.org/abs/2404.01334v1"),a(n)])]),eu,tu,nu,e("p",null,[au,t(": "),e("a",iu,[t("http://arxiv.org/abs/2404.00303v1"),a(n)])]),ou,su,ru,e("p",null,[lu,t(": "),e("a",cu,[t("http://arxiv.org/abs/2404.00267v1"),a(n)])]),hu,du,uu,e("p",null,[gu,t(": "),e("a",pu,[t("http://arxiv.org/abs/2404.00242v1"),a(n)])]),mu,fu,vu,e("p",null,[bu,t(": "),e("a",yu,[t("http://arxiv.org/abs/2404.00216v1"),a(n)])]),Lu,wu,_u,ku,e("p",null,[xu,t(": "),e("a",Mu,[t("http://arxiv.org/abs/2403.20306v1"),a(n)])]),Au,Tu,Cu,e("p",null,[Su,t(": "),e("a",Iu,[t("http://arxiv.org/abs/2403.20288v1"),a(n)])]),zu,Eu,qu,e("p",null,[Pu,t(": "),e("a",Wu,[t("http://arxiv.org/abs/2403.20279v1"),a(n)])]),Gu,Ru,Hu,e("p",null,[Bu,t(": "),e("a",Du,[t("http://arxiv.org/abs/2403.20252v1"),a(n)])]),Ou,Fu,ju,e("p",null,[Ku,t(": "),e("a",Yu,[t("http://arxiv.org/abs/2403.20137v1"),a(n)])]),Nu,Zu,Ju,e("p",null,[Vu,t(": "),e("a",Uu,[t("http://arxiv.org/abs/2403.20046v1"),a(n)])]),Qu,Xu,$u,e("p",null,[eg,t(": "),e("a",tg,[t("http://arxiv.org/abs/2403.19962v1"),a(n)])]),ng,ag,ig,e("p",null,[og,t(": "),e("a",sg,[t("http://arxiv.org/abs/2403.19930v1"),a(n)])]),rg,lg,cg,hg,e("p",null,[dg,t(": "),e("a",ug,[t("http://arxiv.org/abs/2403.19876v1"),a(n)])]),gg,pg,mg,e("p",null,[fg,t(": "),e("a",vg,[t("http://arxiv.org/abs/2403.19857v1"),a(n)])]),bg,yg,Lg,e("p",null,[wg,t(": "),e("a",_g,[t("http://arxiv.org/abs/2403.19506v1"),a(n)])]),kg,xg,Mg,e("p",null,[Ag,t(": "),e("a",Tg,[t("http://arxiv.org/abs/2403.19735v1"),a(n)])]),Cg,Sg,Ig,e("p",null,[zg,t(": "),e("a",Eg,[t("http://arxiv.org/abs/2403.19390v1"),a(n)])]),qg,Pg,Wg,e("p",null,[Gg,t(": "),e("a",Rg,[t("http://arxiv.org/abs/2403.19347v1"),a(n)])]),Hg,Bg,Dg,e("p",null,[Og,t(": "),e("a",Fg,[t("http://arxiv.org/abs/2403.19318v1"),a(n)])]),jg,Kg,Yg,e("p",null,[Ng,t(": "),e("a",Zg,[t("http://arxiv.org/abs/2403.19302v1"),a(n)])]),Jg,Vg,Ug,e("p",null,[Qg,t(": "),e("a",Xg,[t("http://arxiv.org/abs/2403.19114v1"),a(n)])]),$g,ep,tp,e("p",null,[np,t(": "),e("a",ap,[t("http://arxiv.org/abs/2403.19094v1"),a(n)])]),ip,op,sp,rp,e("p",null,[lp,t(": "),e("a",cp,[t("http://arxiv.org/abs/2403.19021v1"),a(n)])]),hp,dp,up,e("p",null,[gp,t(": "),e("a",pp,[t("http://arxiv.org/abs/2403.18721v1"),a(n)])]),mp,fp,vp,e("p",null,[bp,t(": "),e("a",yp,[t("http://arxiv.org/abs/2403.18647v1"),a(n)])]),Lp,wp,_p,e("p",null,[kp,t(": "),e("a",xp,[t("http://arxiv.org/abs/2403.18403v1"),a(n)])]),Mp,Ap,Tp,e("p",null,[Cp,t(": "),e("a",Sp,[t("http://arxiv.org/abs/2403.18349v1"),a(n)])]),Ip,zp,Ep,e("p",null,[qp,t(": "),e("a",Pp,[t("http://arxiv.org/abs/2403.18327v1"),a(n)])]),Wp,Gp,Rp,e("p",null,[Hp,t(": "),e("a",Bp,[t("http://arxiv.org/abs/2403.18249v1"),a(n)])]),Dp,Op,Fp,e("p",null,[jp,t(": "),e("a",Kp,[t("http://arxiv.org/abs/2403.18173v1"),a(n)])]),Yp,Np,Zp,Jp,e("p",null,[Vp,t(": "),e("a",Up,[t("http://arxiv.org/abs/2403.18120v1"),a(n)])]),Qp,Xp,$p,e("p",null,[em,t(": "),e("a",tm,[t("http://arxiv.org/abs/2403.17927v1"),a(n)])]),nm,am,im,e("p",null,[om,t(": "),e("a",sm,[t("http://arxiv.org/abs/2403.17860v1"),a(n)])]),rm,lm,cm,e("p",null,[hm,t(": "),e("a",dm,[t("http://arxiv.org/abs/2403.17856v1"),a(n)])]),um,gm,pm,e("p",null,[mm,t(": "),e("a",fm,[t("http://arxiv.org/abs/2403.17819v1"),a(n)])]),vm])}const _m=i(c,[["render",bm],["__file","LLM.html.vue"]]),km=JSON.parse('{"path":"/posts/paper/LLM.html","title":"LLM","lang":"en-US","frontmatter":{"description":"LLM 2024-04-11 Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation Authors: Jinkyung Park, Pamela Wisniewski, Vivek Singh Link:...","head":[["meta",{"property":"og:url","content":"https://ndesign.world/posts/paper/LLM.html"}],["meta",{"property":"og:site_name","content":"1111"}],["meta",{"property":"og:title","content":"LLM"}],["meta",{"property":"og:description","content":"LLM 2024-04-11 Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation Authors: Jinkyung Park, Pamela Wisniewski, Vivek Singh Link:..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"2024-04-11","slug":"_2024-04-11","link":"#_2024-04-11","children":[]},{"level":2,"title":"2024-04-10","slug":"_2024-04-10","link":"#_2024-04-10","children":[]},{"level":2,"title":"2024-04-09","slug":"_2024-04-09","link":"#_2024-04-09","children":[]},{"level":2,"title":"2024-04-08","slug":"_2024-04-08","link":"#_2024-04-08","children":[]},{"level":2,"title":"2024-04-07","slug":"_2024-04-07","link":"#_2024-04-07","children":[]},{"level":2,"title":"2024-04-06","slug":"_2024-04-06","link":"#_2024-04-06","children":[]},{"level":2,"title":"2024-04-05","slug":"_2024-04-05","link":"#_2024-04-05","children":[]},{"level":2,"title":"2024-04-04","slug":"_2024-04-04","link":"#_2024-04-04","children":[]},{"level":2,"title":"2024-04-03","slug":"_2024-04-03","link":"#_2024-04-03","children":[]},{"level":2,"title":"2024-04-02","slug":"_2024-04-02","link":"#_2024-04-02","children":[]},{"level":2,"title":"2024-04-01","slug":"_2024-04-01","link":"#_2024-04-01","children":[]},{"level":2,"title":"2024-03-31","slug":"_2024-03-31","link":"#_2024-03-31","children":[]},{"level":2,"title":"2024-03-30","slug":"_2024-03-30","link":"#_2024-03-30","children":[]},{"level":2,"title":"2024-03-29","slug":"_2024-03-29","link":"#_2024-03-29","children":[]},{"level":2,"title":"2024-03-28","slug":"_2024-03-28","link":"#_2024-03-28","children":[]},{"level":2,"title":"2024-03-27","slug":"_2024-03-27","link":"#_2024-03-27","children":[]},{"level":2,"title":"2024-03-26","slug":"_2024-03-26","link":"#_2024-03-26","children":[]}],"git":{},"filePathRelative":"posts/paper/LLM.md","autoDesc":true,"excerpt":"\\n<h2>2024-04-11</h2>\\n<h4>Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation</h4>\\n<p><strong>Authors</strong>: Jinkyung Park, Pamela Wisniewski, Vivek Singh</p>\\n<p><strong>Link</strong>: <a href=\\"http://arxiv.org/abs/2404.07926v1\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://arxiv.org/abs/2404.07926v1</a></p>"}');export{_m as comp,km as data};
